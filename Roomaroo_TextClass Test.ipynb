{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d959e63f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to C:\\Users\\Phutawan\n",
      "[nltk_data]     Chonsakorn\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to C:\\Users\\Phutawan\n",
      "[nltk_data]     Chonsakorn\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to C:\\Users\\Phutawan\n",
      "[nltk_data]     Chonsakorn\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "กำลังโหลด Thai2Vec model...\n",
      "กำลังโหลด fastText model...\n",
      "โหลด FastText model สำเร็จ\n",
      "กำลังทำความสะอาดข้อมูล...\n",
      "กำลังสกัดคุณลักษณะจากข้อความ...\n",
      "จำนวนข้อมูลหลังทำความสะอาด: 22493\n",
      "การกระจายของคะแนนหลังทำความสะอาด:\n",
      "rating\n",
      "1    1821\n",
      "2    2191\n",
      "3    2584\n",
      "4    6448\n",
      "5    9449\n",
      "Name: count, dtype: int64\n",
      "\n",
      "การกระจายตามภาษาหลังทำความสะอาด:\n",
      "language\n",
      "english    20491\n",
      "thai        2002\n",
      "Name: count, dtype: int64\n",
      "จำนวนข้อมูลฝึกฝน: 17994\n",
      "จำนวนข้อมูลทดสอบ: 4499\n",
      "กำลังสร้าง word embeddings...\n",
      "กำลังสร้าง TF-IDF features...\n",
      "กำลังสร้าง Count vectors...\n",
      "กำลัง scale vectors (embeddings)...\n",
      "TF-IDF features: 8000\n",
      "Count features: 2000\n",
      "Word embedding features: 300\n",
      "Additional features: 11\n",
      "กำลังรวม features ทั้งหมด...\n",
      "ขนาดของ features รวม - train: (17994, 10311), test: (4499, 10311)\n",
      "แปลง dense สำหรับบางโมเดล...\n",
      "\n",
      "==================================================\n",
      "กำลังฝึกฝนและประเมินผล MODELS\n",
      "==================================================\n",
      "\n",
      "🔧 กำลังฝึกฝน Logistic Regression...\n",
      "✅ Logistic Regression เสร็จสิ้น!\n",
      "   ความแม่นยำการทดสอบ: 0.6026\n",
      "   ความแม่นยำ CV: 0.5979 (±0.0126)\n",
      "   เวลาฝึกฝน: 183.43 วินาที\n",
      "\n",
      "🔧 กำลังฝึกฝน Random Forest...\n",
      "✅ Random Forest เสร็จสิ้น!\n",
      "   ความแม่นยำการทดสอบ: 0.5719\n",
      "   ความแม่นยำ CV: 0.5730 (±0.0086)\n",
      "   เวลาฝึกฝน: 32.08 วินาที\n",
      "\n",
      "🔧 กำลังฝึกฝน Linear SVM...\n"
     ]
    }
   ],
   "source": [
    "### **0. Install Required Libraries**\n",
    "# !pip install numpy==1.23.5\n",
    "# !pip install --upgrade gensim\n",
    "# !pip install --upgrade pythainlp\n",
    "# !pip install emoji\n",
    "# !pip install fasttext-wheel  # สำหรับ fastText embeddings\n",
    "\n",
    "### **1. Import Libraries**\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import time\n",
    "import re\n",
    "import string\n",
    "import emoji\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Thai NLP - เครื่องมือประมวลผลภาษาไทย\n",
    "from pythainlp.tokenize import word_tokenize\n",
    "from pythainlp.corpus import thai_negations\n",
    "from pythainlp.word_vector import WordVector\n",
    "\n",
    "# Machine Learning - เครื่องมือแมชชีนเลิร์นนิง\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "from scipy.sparse import hstack, csr_matrix\n",
    "from collections import Counter\n",
    "\n",
    "# FastText - เครื่องมือสำหรับ word embedding\n",
    "import fasttext\n",
    "import fasttext.util\n",
    "\n",
    "# NLTK สำหรับภาษาอังกฤษ\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize as nltk_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "# ดาวน์โหลด NLTK data (รันครั้งแรกเท่านั้น)\n",
    "try:\n",
    "    nltk.data.find('tokenizers/punkt')\n",
    "    nltk.data.find('corpora/stopwords')\n",
    "    nltk.data.find('corpora/wordnet')\n",
    "except LookupError:\n",
    "    nltk.download('punkt')\n",
    "    nltk.download('stopwords')\n",
    "    nltk.download('wordnet')\n",
    "\n",
    "## **2. Load and Explore Data**\n",
    "# กำหนดการแสดงผลภาษาไทย\n",
    "import matplotlib\n",
    "matplotlib.rcParams['font.family'] = 'DejaVu Sans'\n",
    "\n",
    "# โหลด dataset ภาษาไทย (รีวิวหอพัก)\n",
    "df_thai = pd.read_csv(\"Data/dorm_reviews.csv\")\n",
    "# คาดว่าไฟล์ไทยมีคอลัมน์ text, rating อยู่แล้ว\n",
    "if 'dormitory_id' in df_thai.columns:\n",
    "    df_thai = df_thai.drop([c for c in ['dormitory_id', 'user_id'] if c in df_thai.columns], axis=1)\n",
    "df_thai = df_thai.rename(columns={'Review':'text','Rating':'rating'})\n",
    "df_thai = df_thai[['text','rating']].copy()\n",
    "df_thai['language'] = 'thai'\n",
    "\n",
    "# โหลด dataset ภาษาอังกฤษ (รีวิวโรงแรม)\n",
    "df_english = pd.read_csv(\"Data/tripadvisor_hotel_reviews.csv\")\n",
    "# ให้เหลือคอลัมน์ text, rating\n",
    "if {'Review','Rating'}.issubset(df_english.columns):\n",
    "    df_english = df_english[['Review','Rating']].copy()\n",
    "    df_english.columns = ['text','rating']\n",
    "else:\n",
    "    # เผื่อไฟล์ใช้ชื่อคอลัมน์อื่น\n",
    "    possible_text = [c for c in df_english.columns if c.lower() in ['review','text','comment','content']]\n",
    "    possible_rating = [c for c in df_english.columns if c.lower() in ['rating','score','stars']]\n",
    "    df_english = df_english[[possible_text[0], possible_rating[0]]].copy()\n",
    "    df_english.columns = ['text','rating']\n",
    "df_english['language'] = 'english'\n",
    "\n",
    "# รวมสองชุดเป็น df (แก้ NameError เดิม)\n",
    "df = pd.concat([df_thai, df_english], ignore_index=True)\n",
    "\n",
    "# ทำ rating ให้เป็น int 1..5\n",
    "df['rating'] = pd.to_numeric(df['rating'], errors='coerce').clip(1,5).astype(int)\n",
    "\n",
    "### **3. โหลด Word Embedding Models**\n",
    "# โหลด Thai2Vec model สำหรับภาษาไทย\n",
    "print(\"กำลังโหลด Thai2Vec model...\")\n",
    "thai2fit_model = WordVector(model_name=\"thai2fit_wv\").get_model()\n",
    "\n",
    "# ดาวน์โหลดและโหลด fastText model สำหรับภาษาอังกฤษ\n",
    "print(\"กำลังโหลด fastText model...\")\n",
    "fasttext_model = fasttext.load_model('cc.en.300.bin')\n",
    "print(\"โหลด FastText model สำเร็จ\")\n",
    "\n",
    "def enhanced_sentence_vectorizer(text, dim=300):\n",
    "    \"\"\"\n",
    "    ฟังก์ชันแปลงประโยคเป็น vector โดยใช้ Thai2Vec และ fastText\n",
    "    \"\"\"\n",
    "    if not isinstance(text, str):\n",
    "        text = \"\"\n",
    "    is_thai = any(ord(char) >= 3584 and ord(char) <= 3711 for char in text)\n",
    "    words = word_tokenize(text, engine=\"newmm\") if is_thai else nltk_tokenize(text.lower())\n",
    "\n",
    "    vec = np.zeros(dim, dtype=np.float32)\n",
    "    word_count = 0\n",
    "    total_words = len(words)\n",
    "    \n",
    "    for i, word in enumerate(words):\n",
    "        if word.strip():\n",
    "            position_weight = 1.0 + (i / max(total_words, 1)) * 0.5\n",
    "            word_vector = None\n",
    "            \n",
    "            if is_thai:\n",
    "                if word in thai2fit_model:\n",
    "                    word_vector = thai2fit_model[word] * position_weight\n",
    "                else:\n",
    "                    char_vec, char_count = np.zeros(dim, dtype=np.float32), 0\n",
    "                    for char in word:\n",
    "                        if char in thai2fit_model:\n",
    "                            char_vec += thai2fit_model[char]\n",
    "                            char_count += 1\n",
    "                    if char_count > 0:\n",
    "                        word_vector = (char_vec / char_count) * position_weight\n",
    "            else:\n",
    "                try:\n",
    "                    word_vector = fasttext_model.get_word_vector(word) * position_weight\n",
    "                except Exception:\n",
    "                    pass\n",
    "            \n",
    "            if word_vector is not None:\n",
    "                vec += word_vector\n",
    "                word_count += 1\n",
    "    \n",
    "    return vec / max(word_count, 1)\n",
    "\n",
    "### **4. ฟังก์ชันทำความสะอาดข้อความและการสกัดคุณลักษณะ**\n",
    "def clean_text_thai(text):\n",
    "    if not isinstance(text, str):\n",
    "        return \"\"\n",
    "    from pythainlp.corpus import thai_stopwords\n",
    "    thai_stop_words = list(thai_stopwords())\n",
    "    important_words = [\n",
    "        \"ไม่\",\"ไม่มี\",\"ไม่ได้\",\"ไม่ค่อย\",\"ไม่เคย\",\"ห้าม\",\"ยังไม่\",\"ไม่ยอม\",\n",
    "        \"ดี\",\"ดีมาก\",\"สะอาด\",\"เย็น\",\"กว้าง\",\"ใหม่\",\"สวย\",\"น่าอยู่\",\"สบาย\",\n",
    "        \"ชอบ\",\"ประทับใจ\",\"สะดวก\",\"ปลอดภัย\",\"คุ้ม\",\"คุ้มค่า\",\"เงียบ\",\"ครบ\",\n",
    "        \"พอใจ\",\"เร็ว\",\"โอเค\",\"โอเคเลย\",\"เยี่ยม\",\"ถูกใจ\",\"ทำเลดี\",\"ใกล้\",\"ครบครัน\",\n",
    "        \"แย่\",\"ไม่ดี\",\"เหม็น\",\"ร้อน\",\"แคบ\",\"เก่า\",\"สกปรก\",\"พัง\",\"เสียงดัง\",\n",
    "        \"แพง\",\"ไม่ชอบ\",\"แออัด\",\"รก\",\"อันตราย\",\"ช้า\",\"ผิดหวัง\",\"ห่วย\",\"เฟล\",\n",
    "        \"กาก\",\"ไม่คุ้ม\",\"ไกล\",\"รั่ว\",\"ทรุด\",\"ทรุดโทรม\",\"เสื่อม\",\n",
    "        \"มาก\",\"สุดๆ\",\"เยอะ\",\"น้อย\",\"ที่สุด\",\"หลาย\",\"ทุก\",\"เกิน\",\"จัด\",\"โคตร\",\n",
    "        \"มากๆ\",\"สุดยอด\",\"ธรรมดา\",\"พอใช้\",\"บ่อย\",\"ตลอด\",\"เวอร์\",\n",
    "        \"แอร์\",\"น้ำ\",\"ไฟ\",\"ห้องน้ำ\",\"เตียง\",\"ฝักบัว\",\"เน็ต\",\"ไวไฟ\",\"ไฟฟ้า\",\n",
    "        \"ประปา\",\"เฟอร์\",\"ลิฟต์\",\"ที่จอด\",\"จอดรถ\",\"ซักผ้า\",\"ตู้เย็น\",\"ทีวี\",\n",
    "        \"จาน\",\"ไมโครเวฟ\",\"เตา\",\"น้ำอุ่น\",\"ผ้าปู\",\"โต๊ะ\",\"เก้าอี้\",\"ตู้\",\"ชั้นวาง\",\n",
    "        \"ปลั๊ก\",\"สัญญาณ\",\n",
    "        \"เสียง\",\"มด\",\"แมลง\",\"แมลงสาบ\",\"หนู\",\"ยุง\",\"ฝุ่น\",\"กลิ่น\",\"เพื่อนบ้าน\",\n",
    "        \"ข้างห้อง\",\"ข้างนอก\",\"ถนน\",\"ทางเดิน\",\"ลานจอด\",\"ชั้นบน\",\"บันได\",\"กำแพง\",\n",
    "        \"ดูแล\",\"บริการ\",\"ซ่อม\",\"แก้ไข\",\"จัดการ\",\"พนักงาน\",\"แม่บ้าน\",\"รปภ\",\n",
    "        \"เจ้าของ\",\"นิติ\",\"กฎ\",\"ระเบียบ\",\"ค่าเช่า\",\"ค่าไฟ\",\"ค่าน้ำ\",\"ค่าส่วนกลาง\",\n",
    "        \"มัดจำ\",\"ประกัน\",\"สัญญา\",\"ฝากของ\",\"รับพัสดุ\",\"คีย์การ์ด\",\"ล็อค\",\"รอนาน\",\n",
    "        \"ไม่มาดู\",\"ไม่ซ่อม\",\n",
    "        \"แต่\",\"แต่ว่า\",\"ถึงแม้\",\"อย่างไรก็ตาม\",\"เพราะ\",\"เพราะว่า\",\"เนื่องจาก\",\n",
    "        \"คือ\",\"ก็คือ\",\"ส่วน\",\"นอกจากนี้\",\"ที่จริง\",\"จริงๆ\",\"ก็\",\"แม้\",\"ที่\",\n",
    "        \"ตอนแรก\",\"พอดี\",\"แล้วก็\"\n",
    "    ]\n",
    "    custom_stop_words = [w for w in thai_stop_words if w not in important_words]\n",
    "    punct = string.punctuation.replace('!', '').replace('?', '').replace('.', '')\n",
    "    text = emoji.replace_emoji(text, replace=\"\")\n",
    "    text = ''.join(ch if ch not in punct else ' ' for ch in text)\n",
    "    text = re.sub(r'([ก-๙a-zA-Z])\\1{2,}', r'\\1\\1', text)\n",
    "    text = re.sub(r'\\s+', \" \", text).strip().lower()\n",
    "    words = word_tokenize(text, engine='newmm')\n",
    "    words = [w for w in words if w not in custom_stop_words]\n",
    "    return ' '.join(words)\n",
    "\n",
    "def clean_text_english(text):\n",
    "    if not isinstance(text, str):\n",
    "        return \"\"\n",
    "    english_stop_words = set(stopwords.words('english'))\n",
    "    important_english_words = {\n",
    "        'not','no','never','nothing','nowhere','neither','nobody','none',\n",
    "        'good','great','excellent','amazing','wonderful','perfect','love',\n",
    "        'best','nice','clean','comfortable','convenient','recommend',\n",
    "        'bad','terrible','awful','horrible','worst','hate','dirty',\n",
    "        'uncomfortable','expensive','cheap','noisy','small',\n",
    "        'very','really','extremely','quite','pretty','too','so','absolutely'\n",
    "    }\n",
    "    custom_stop_words = english_stop_words - important_english_words\n",
    "    punct = string.punctuation.replace('!', '').replace('?', '').replace('.', '')\n",
    "    text = emoji.replace_emoji(text, replace=\"\")\n",
    "    text = ''.join(ch if ch not in punct else ' ' for ch in text)\n",
    "    text = re.sub(r'([a-zA-Z])\\1{2,}', r'\\1\\1', text)\n",
    "    text = re.sub(r'\\s+', \" \", text).strip().lower()\n",
    "    words = nltk_tokenize(text)\n",
    "    lem = WordNetLemmatizer()\n",
    "    words = [lem.lemmatize(w) for w in words if w not in custom_stop_words and w.isalpha()]\n",
    "    return ' '.join(words)\n",
    "\n",
    "def clean_text(text):\n",
    "    is_thai = any(ord(c) >= 3584 and ord(c) <= 3711 for c in str(text))\n",
    "    return clean_text_thai(text) if is_thai else clean_text_english(text)\n",
    "\n",
    "def extract_features(text):\n",
    "    is_thai = any(ord(c) >= 3584 and ord(c) <= 3711 for c in str(text))\n",
    "    words = word_tokenize(text, engine='newmm') if is_thai else nltk_tokenize(text.lower())\n",
    "    word_count = len(words)\n",
    "    features = {\n",
    "        'exclamation_count': text.count('!'),\n",
    "        'question_count': text.count('?'),\n",
    "        'sentence_count': text.count('.') + 1,\n",
    "        'word_count': word_count,\n",
    "        'avg_word_length': sum(len(w) for w in words) / max(word_count, 1),\n",
    "        'text_length': len(text)\n",
    "    }\n",
    "    word_counts = Counter(words)\n",
    "    repeated_words = sum(1 for cnt in word_counts.values() if cnt > 1)\n",
    "    features['repeated_words_ratio'] = repeated_words / max(word_count, 1)\n",
    "    if is_thai:\n",
    "        negation_words = set(thai_negations())\n",
    "        features['negation_count'] = sum(1 for w in words if w in negation_words)\n",
    "    else:\n",
    "        english_negations = {'not','no','never','nothing','nobody','none','neither','nowhere'}\n",
    "        features['negation_count'] = sum(1 for w in words if w in english_negations)\n",
    "    features['punctuation_ratio'] = len([c for c in text if c in string.punctuation]) / max(len(text), 1)\n",
    "    features['words_per_sentence'] = word_count / max(features['sentence_count'], 1)\n",
    "    english_words = sum(1 for w in words if w.isascii() and w.isalpha())\n",
    "    features['english_ratio'] = english_words / max(word_count, 1)\n",
    "    return features\n",
    "\n",
    "### **5. ทำความสะอาดและเตรียมข้อมูล**\n",
    "print(\"กำลังทำความสะอาดข้อมูล...\")\n",
    "df['cleaned_review'] = df['text'].astype(str).apply(clean_text)\n",
    "\n",
    "def get_word_count(text):\n",
    "    is_thai = any(ord(char) >= 3584 and ord(char) <= 3711 for char in str(text))\n",
    "    return len(word_tokenize(text, engine='newmm')) if is_thai else len(nltk_tokenize(text))\n",
    "\n",
    "df = df[df['cleaned_review'].apply(get_word_count) > 3]\n",
    "df = df.drop_duplicates(subset=['cleaned_review'])\n",
    "\n",
    "print(\"กำลังสกัดคุณลักษณะจากข้อความ...\")\n",
    "feature_columns = ['cleaned_review']\n",
    "feature_names = ['exclamation_count','question_count','sentence_count','word_count',\n",
    "                 'avg_word_length','repeated_words_ratio','negation_count',\n",
    "                 'punctuation_ratio','text_length','words_per_sentence','english_ratio']\n",
    "\n",
    "for f in feature_names:\n",
    "    df[f] = df['cleaned_review'].apply(lambda x: extract_features(x)[f])\n",
    "\n",
    "feature_columns.extend(feature_names)\n",
    "\n",
    "print(f\"จำนวนข้อมูลหลังทำความสะอาด: {len(df)}\")\n",
    "print(\"การกระจายของคะแนนหลังทำความสะอาด:\")\n",
    "print(df['rating'].value_counts().sort_index())\n",
    "print(\"\\nการกระจายตามภาษาหลังทำความสะอาด:\")\n",
    "print(df['language'].value_counts())\n",
    "\n",
    "### **6. แบ่งข้อมูล**\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    df[feature_columns], df['rating'],\n",
    "    test_size=0.2, random_state=42, stratify=df['rating'] if df['rating'].nunique()>1 else None\n",
    ")\n",
    "print(f\"จำนวนข้อมูลฝึกฝน: {len(X_train)}\")\n",
    "print(f\"จำนวนข้อมูลทดสอบ: {len(X_test)}\")\n",
    "\n",
    "### **7. สร้าง Feature Vectors**\n",
    "def custom_tokenizer_mixed(text):\n",
    "    is_thai = any(ord(char) >= 3584 and ord(char) <= 3711 for char in str(text))\n",
    "    return word_tokenize(text, engine='newmm') if is_thai else nltk_tokenize(text.lower())\n",
    "\n",
    "print(\"กำลังสร้าง word embeddings...\")\n",
    "X_train_vectors = np.array([enhanced_sentence_vectorizer(t) for t in X_train['cleaned_review']], dtype=np.float32)\n",
    "X_test_vectors  = np.array([enhanced_sentence_vectorizer(t) for t in X_test['cleaned_review']], dtype=np.float32)\n",
    "\n",
    "print(\"กำลังสร้าง TF-IDF features...\")\n",
    "tfidf_vectorizer = TfidfVectorizer(\n",
    "    tokenizer=custom_tokenizer_mixed,\n",
    "    max_features=8000,\n",
    "    ngram_range=(1,3),\n",
    "    min_df=2,\n",
    "    max_df=0.85,\n",
    "    use_idf=True,\n",
    "    smooth_idf=True,\n",
    "    sublinear_tf=True\n",
    ")\n",
    "\n",
    "print(\"กำลังสร้าง Count vectors...\")\n",
    "count_vectorizer = CountVectorizer(\n",
    "    tokenizer=custom_tokenizer_mixed,\n",
    "    max_features=2000,\n",
    "    ngram_range=(1,2),\n",
    "    min_df=3,\n",
    "    max_df=0.85\n",
    ")\n",
    "\n",
    "X_train_tfidf = tfidf_vectorizer.fit_transform(X_train['cleaned_review'])\n",
    "X_test_tfidf  = tfidf_vectorizer.transform(X_test['cleaned_review'])\n",
    "X_train_count = count_vectorizer.fit_transform(X_train['cleaned_review'])\n",
    "X_test_count  = count_vectorizer.transform(X_test['cleaned_review'])\n",
    "\n",
    "print(\"กำลัง scale vectors (embeddings)...\")\n",
    "scaler = StandardScaler(with_mean=False)\n",
    "X_train_vectors_sparse = csr_matrix(X_train_vectors)\n",
    "X_test_vectors_sparse  = csr_matrix(X_test_vectors)\n",
    "X_train_vectors_scaled = scaler.fit_transform(X_train_vectors_sparse)\n",
    "X_test_vectors_scaled  = scaler.transform(X_test_vectors_sparse)\n",
    "\n",
    "numerical_features = [c for c in feature_columns if c != 'cleaned_review']\n",
    "X_train_additional = X_train[numerical_features].values\n",
    "X_test_additional  = X_test[numerical_features].values\n",
    "\n",
    "features_scaler = StandardScaler()\n",
    "X_train_additional_scaled = features_scaler.fit_transform(X_train_additional)\n",
    "X_test_additional_scaled  = features_scaler.transform(X_test_additional)\n",
    "\n",
    "print(f\"TF-IDF features: {X_train_tfidf.shape[1]}\")\n",
    "print(f\"Count features: {X_train_count.shape[1]}\")\n",
    "print(f\"Word embedding features: {X_train_vectors.shape[1]}\")\n",
    "print(f\"Additional features: {X_train_additional.shape[1]}\")\n",
    "\n",
    "### **8. รวม Features ทั้งหมด**\n",
    "print(\"กำลังรวม features ทั้งหมด...\")\n",
    "X_train_combined = hstack([\n",
    "    X_train_tfidf,\n",
    "    X_train_count,\n",
    "    X_train_vectors_scaled,\n",
    "    csr_matrix(X_train_additional_scaled)\n",
    "], format='csr')\n",
    "\n",
    "X_test_combined = hstack([\n",
    "    X_test_tfidf,\n",
    "    X_test_count,\n",
    "    X_test_vectors_scaled,\n",
    "    csr_matrix(X_test_additional_scaled)\n",
    "], format='csr')\n",
    "\n",
    "print(f\"ขนาดของ features รวม - train: {X_train_combined.shape}, test: {X_test_combined.shape}\")\n",
    "\n",
    "# สำรองแบบ dense สำหรับโมเดลที่ไม่รองรับ sparse\n",
    "print(\"แปลง dense สำหรับบางโมเดล...\")\n",
    "X_train_combined_dense = X_train_combined.toarray()\n",
    "X_test_combined_dense  = X_test_combined.toarray()\n",
    "\n",
    "# สำหรับ Naive Bayes ต้องไม่เป็นลบ\n",
    "X_train_nb = np.abs(X_train_combined_dense)\n",
    "X_test_nb  = np.abs(X_test_combined_dense)\n",
    "\n",
    "### **9. กำหนด Models**\n",
    "models = {\n",
    "    'Logistic Regression': LogisticRegression(random_state=42, max_iter=1000, C=0.2,\n",
    "                                              class_weight='balanced', solver='saga',\n",
    "                                              multi_class='multinomial'),\n",
    "    'Random Forest': RandomForestClassifier(random_state=42, n_jobs=-1,\n",
    "                                            n_estimators=200, max_depth=15,\n",
    "                                            min_samples_split=5, min_samples_leaf=2,\n",
    "                                            class_weight='balanced'),\n",
    "    'Linear SVM': LinearSVC(random_state=42, max_iter=2000, C=0.5,\n",
    "                            class_weight='balanced', dual=False),\n",
    "    'Multinomial Naive Bayes': MultinomialNB(alpha=1.0, fit_prior=True)\n",
    "}\n",
    "\n",
    "### **10. ฝึกฝนและประเมินผล Models**\n",
    "results = {}\n",
    "best_models = {}\n",
    "training_times = {}\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"กำลังฝึกฝนและประเมินผล MODELS\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "for model_name, model in models.items():\n",
    "    print(f\"\\n🔧 กำลังฝึกฝน {model_name}...\")\n",
    "    start_time = time.time()\n",
    "    \n",
    "    # เลือกชุดฟีเจอร์ที่เหมาะสม\n",
    "    if model_name == 'Multinomial Naive Bayes':\n",
    "        Xtr, Xte = X_train_nb, X_test_nb\n",
    "    elif model_name == 'Random Forest':\n",
    "        # Tree-based ไม่รับ sparse -> ใช้ dense\n",
    "        Xtr, Xte = X_train_combined_dense, X_test_combined_dense\n",
    "    else:\n",
    "        Xtr, Xte = X_train_combined, X_test_combined\n",
    "    \n",
    "    model.fit(Xtr, y_train)\n",
    "    best_models[model_name] = model\n",
    "    training_time = time.time() - start_time\n",
    "    training_times[model_name] = training_time\n",
    "    \n",
    "    y_pred = model.predict(Xte)\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "\n",
    "    # cross_val_score: ใช้ฟีเจอร์เดียวกับตอน train\n",
    "    try:\n",
    "        cv_scores = cross_val_score(model, Xtr, y_train, cv=5, scoring='accuracy')\n",
    "    except Exception:\n",
    "        # เผื่อบางโมเดลไม่เหมาะกับ CV ในสภาพนี้\n",
    "        cv_scores = np.array([accuracy])\n",
    "    \n",
    "    results[model_name] = {\n",
    "        'accuracy': accuracy,\n",
    "        'cv_mean': float(np.mean(cv_scores)),\n",
    "        'cv_std': float(np.std(cv_scores)),\n",
    "        'predictions': y_pred,\n",
    "        'training_time': training_time\n",
    "    }\n",
    "    \n",
    "    print(f\"✅ {model_name} เสร็จสิ้น!\")\n",
    "    print(f\"   ความแม่นยำการทดสอบ: {accuracy:.4f}\")\n",
    "    print(f\"   ความแม่นยำ CV: {np.mean(cv_scores):.4f} (±{np.std(cv_scores):.4f})\")\n",
    "    print(f\"   เวลาฝึกฝน: {training_time:.2f} วินาที\")\n",
    "\n",
    "### **11. เปรียบเทียบผลลัพธ์และการแสดงผล**\n",
    "results_df = pd.DataFrame({\n",
    "    'Model': list(results.keys()),\n",
    "    'Test Accuracy': [results[m]['accuracy'] for m in results.keys()],\n",
    "    'CV Mean': [results[m]['cv_mean'] for m in results.keys()],\n",
    "    'CV Std': [results[m]['cv_std'] for m in results.keys()],\n",
    "    'Training Time (s)': [results[m]['training_time'] for m in results.keys()]\n",
    "})\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"ผลการเปรียบเทียบ MODELS\")\n",
    "print(\"=\"*60)\n",
    "print(results_df.round(4))\n",
    "\n",
    "best_model_name = results_df.loc[results_df['Test Accuracy'].idxmax(), 'Model']\n",
    "print(f\"\\n🏆 โมเดลที่มีประสิทธิภาพดีที่สุด: {best_model_name}\")\n",
    "print(f\"   ความแม่นยำการทดสอบ: {results[best_model_name]['accuracy']:.4f}\")\n",
    "\n",
    "### **12. การแสดงผลแบบกราฟ**\n",
    "plt.rcParams['font.family'] = ['DejaVu Sans']\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
    "\n",
    "axes[0, 0].bar(results_df['Model'], results_df['Test Accuracy'])\n",
    "axes[0, 0].set_title('การเปรียบเทียบความแม่นยำการทดสอบ', fontsize=14, fontweight='bold')\n",
    "axes[0, 0].set_ylabel('ความแม่นยำ'); axes[0, 0].tick_params(axis='x', rotation=45); axes[0, 0].grid(True, alpha=0.3)\n",
    "for i, v in enumerate(results_df['Test Accuracy']):\n",
    "    axes[0, 0].text(i, v + 0.005, f'{v:.3f}', ha='center', fontweight='bold')\n",
    "\n",
    "axes[0, 1].errorbar(range(len(results_df)), results_df['CV Mean'],\n",
    "                    yerr=results_df['CV Std'], fmt='o-', capsize=5, linewidth=2)\n",
    "axes[0, 1].set_title('ความแม่นยำ Cross-Validation (เฉลี่ย ± ส่วนเบี่ยงเบนมาตรฐาน)', fontsize=14, fontweight='bold')\n",
    "axes[0, 1].set_ylabel('ความแม่นยำ CV'); axes[0, 1].set_xticks(range(len(results_df)))\n",
    "axes[0, 1].set_xticklabels(results_df['Model'], rotation=45); axes[0, 1].grid(True, alpha=0.3)\n",
    "\n",
    "bars = axes[1, 0].bar(results_df['Model'], results_df['Training Time (s)'])\n",
    "axes[1, 0].set_title('การเปรียบเทียบเวลาฝึกฝน', fontsize=14, fontweight='bold')\n",
    "axes[1, 0].set_ylabel('เวลา (วินาที)'); axes[1, 0].tick_params(axis='x', rotation=45); axes[1, 0].grid(True, alpha=0.3)\n",
    "for bar, time_val in zip(bars, results_df['Training Time (s)']):\n",
    "    axes[1, 0].text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.1, f'{time_val:.1f}s', ha='center', fontweight='bold')\n",
    "\n",
    "# Radar\n",
    "categories = ['ความแม่นยำทดสอบ', 'ความแม่นยำ CV', 'ความเร็ว (กลับหัว)']\n",
    "fig2, ax = plt.subplots(figsize=(8, 8), subplot_kw=dict(projection='polar'))\n",
    "normalized_accuracy = results_df['Test Accuracy'] / max(1e-9, results_df['Test Accuracy'].max())\n",
    "normalized_cv = results_df['CV Mean'] / max(1e-9, results_df['CV Mean'].max())\n",
    "normalized_speed = (1 / (results_df['Training Time (s)'] + 0.1))\n",
    "normalized_speed = normalized_speed / max(1e-9, normalized_speed.max())\n",
    "angles = np.linspace(0, 2*np.pi, len(categories), endpoint=False).tolist(); angles += angles[:1]\n",
    "base_colors = ['C0','C1','C2','C3','C4','C5','C6','C7','C8','C9']\n",
    "for i, model in enumerate(results_df['Model']):\n",
    "    values = [normalized_accuracy.iloc[i], normalized_cv.iloc[i], normalized_speed.iloc[i]]; values += values[:1]\n",
    "    ax.plot(angles, values, 'o-', linewidth=2, label=model, color=base_colors[i % len(base_colors)])\n",
    "    ax.fill(angles, values, alpha=0.25, color=base_colors[i % len(base_colors)])\n",
    "ax.set_xticks(angles[:-1]); ax.set_xticklabels(categories); ax.set_ylim(0, 1)\n",
    "ax.set_title('การเปรียบเทียบประสิทธิภาพโมเดล (ปรับมาตรฐาน)', fontweight='bold', size=14)\n",
    "ax.legend(loc='upper right', bbox_to_anchor=(1.2, 1.0)); ax.grid(True)\n",
    "axes[1, 1].remove()\n",
    "plt.tight_layout(); plt.show()\n",
    "\n",
    "### **13. รายงานการจำแนกประเภทโดยละเอียด**\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"รายงานการจำแนกประเภทโดยละเอียด\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# ทำให้ปลอดภัยหากบางคลาสไม่มีใน y_test\n",
    "unique_labels = sorted(df['rating'].unique().tolist())\n",
    "target_names = [f'คะแนน {i}' for i in unique_labels]\n",
    "\n",
    "for model_name in results.keys():\n",
    "    print(f\"\\n📊 {model_name}\")\n",
    "    print(\"-\" * 40)\n",
    "    model = models[model_name]\n",
    "    params_show = ['C','class_weight','solver','multi_class','n_estimators',\n",
    "                   'max_depth','min_samples_split','min_samples_leaf','dual',\n",
    "                   'alpha','fit_prior']\n",
    "    param_desc = {\n",
    "        'C':'ค่า regularization parameter','class_weight':'การปรับน้ำหนักคลาส',\n",
    "        'solver':'อัลกอริทึมการหาค่าที่เหมาะสม','multi_class':'วิธีการจัดการหลายคลาส',\n",
    "        'n_estimators':'จำนวน decision trees','max_depth':'ความลึกสูงสุดของต้นไม้',\n",
    "        'min_samples_split':'จำนวนตัวอย่างขั้นต่ำในการแยก node',\n",
    "        'min_samples_leaf':'จำนวนตัวอย่างขั้นต่ำใน leaf node',\n",
    "        'dual':'การใช้ dual formulation','alpha':'ค่า smoothing parameter',\n",
    "        'fit_prior':'การใช้ prior probability'\n",
    "    }\n",
    "    for p, v in model.get_params().items():\n",
    "        if p in params_show:\n",
    "            print(f\"  {p}: {v} # {param_desc.get(p,'')}\")\n",
    "    print(f\"\\nรายงานการจำแนกประเภท:\")\n",
    "    # ใช้ labels และ target_names ให้สอดคล้องกัน\n",
    "    print(classification_report(y_test, results[model_name]['predictions'],\n",
    "                                labels=unique_labels, target_names=target_names, zero_division=0))\n",
    "\n",
    "### **14. Confusion Matrices**\n",
    "rows = int(np.ceil(len(results)/2))\n",
    "cols = 2 if len(results) > 1 else 1\n",
    "fig, axes = plt.subplots(rows, cols, figsize=(15, 6*rows))\n",
    "axes = np.array(axes).reshape(-1)\n",
    "for i, (model_name, result) in enumerate(results.items()):\n",
    "    cm = confusion_matrix(y_test, result['predictions'], labels=unique_labels)\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
    "                xticklabels=[str(l) for l in unique_labels],\n",
    "                yticklabels=[str(l) for l in unique_labels],\n",
    "                ax=axes[i])\n",
    "    axes[i].set_xlabel(\"คะแนนที่ทำนาย\"); axes[i].set_ylabel(\"คะแนนจริง\")\n",
    "    axes[i].set_title(f\"{model_name}\\nความแม่นยำ: {result['accuracy']:.3f}\")\n",
    "plt.tight_layout(); plt.savefig(\"confusion_matrices_comparison.png\", dpi=300, bbox_inches='tight'); plt.show()\n",
    "\n",
    "### **15. ฟังก์ชันทำนายผลที่ปรับปรุง**\n",
    "def predict_review_enhanced(text, model_name='best'):\n",
    "    if model_name == 'best':\n",
    "        model_name = best_model_name\n",
    "    model = best_models[model_name]\n",
    "    cleaned_text = clean_text(text)\n",
    "    features = extract_features(cleaned_text)\n",
    "\n",
    "    review_vector = enhanced_sentence_vectorizer(cleaned_text).reshape(1, -1)\n",
    "    review_vector_sparse = csr_matrix(review_vector)\n",
    "    review_vector_scaled = scaler.transform(review_vector_sparse)\n",
    "\n",
    "    review_tfidf = tfidf_vectorizer.transform([cleaned_text])\n",
    "    review_count = count_vectorizer.transform([cleaned_text])\n",
    "\n",
    "    additional_features = np.array([[\n",
    "        features['exclamation_count'], features['question_count'],\n",
    "        features['sentence_count'], features['word_count'],\n",
    "        features['avg_word_length'], features['repeated_words_ratio'],\n",
    "        features['negation_count'], features['punctuation_ratio'],\n",
    "        features['text_length'], features['words_per_sentence'],\n",
    "        features['english_ratio']\n",
    "    ]], dtype=np.float32)\n",
    "    additional_features_scaled = features_scaler.transform(additional_features)\n",
    "\n",
    "    if model_name in ['Multinomial Naive Bayes','Random Forest']:\n",
    "        # ใช้เวอร์ชัน dense สำหรับโมเดลที่ต้องการ non-negative/dense\n",
    "        review_combined = hstack([\n",
    "            review_tfidf, review_count, review_vector_scaled, csr_matrix(additional_features_scaled)\n",
    "        ], format='csr').toarray()\n",
    "        if model_name == 'Multinomial Naive Bayes':\n",
    "            review_combined = np.abs(review_combined)\n",
    "    else:\n",
    "        review_combined = hstack([\n",
    "            review_tfidf, review_count, review_vector_scaled, csr_matrix(additional_features_scaled)\n",
    "        ], format='csr')\n",
    "\n",
    "    pred = model.predict(review_combined)[0]\n",
    "    if hasattr(model, 'predict_proba'):\n",
    "        probs = model.predict_proba(review_combined)[0]\n",
    "    else:\n",
    "        # LinearSVC\n",
    "        scores = model.decision_function(review_combined)[0]\n",
    "        # ปรับ softmax แบบง่าย\n",
    "        exps = np.exp(scores - np.max(scores))\n",
    "        probs = exps / np.sum(exps)\n",
    "    return int(pred), probs\n",
    "\n",
    "### **16. ทดสอบกับรีวิวจริง**\n",
    "def print_prediction_results_enhanced(text, actual_rating, model_name='best'):\n",
    "    predicted_class, confidences = predict_review_enhanced(text, model_name)\n",
    "    is_correct = int(predicted_class) == int(actual_rating)\n",
    "    print(f\"\\nรีวิว: {text[:100]}...\")\n",
    "    print(f\"โมเดล: {model_name}\")\n",
    "    print(f\"คะแนนจริง: {actual_rating}/5\")\n",
    "    print(f\"คะแนนที่ทำนาย: {predicted_class}/5\")\n",
    "    print(f\"ผลลัพธ์: {'✅ ถูกต้อง' if is_correct else '❌ ผิดพลาด'}\")\n",
    "    print(\"ความมั่นใจ:\")\n",
    "    # ทำ mapping ตามลำดับคลาสที่โมเดลใช้ (สมมติเรียง 1..5)\n",
    "    for rating_idx, confidence in enumerate(confidences, 1):\n",
    "        print(f\"  ⭐ {rating_idx}: {confidence * 100:.1f}%\")\n",
    "    return predicted_class, int(actual_rating)\n",
    "\n",
    "test_reviews = [\n",
    "    [\"ไม่แนะนำเลยค่ะ หอนี้ หลอกเอาเงินชัดๆ ในรูปสวยมาก แต่พอเข้าไปอยู่จริงสภาพห้องทรุดโทรมมาก ตู้เสื้อผ้าพังตั้งแต่วันแรกที่ย้ายเข้า เตียงก็เก่ามากนอนแล้วปวดหลัง ฝักบัวน้ำก็ไหลแค่ซิกๆ ไม่เคยมาซ่อมให้สักที ขอย้ายออกก็ไม่คืนเงินมัดจำ เสียความรู้สึกมากค่ะ\", 1],\n",
    "    [\"หอพักราคาก็โอเคนะ ไม่แพงมาก แต่มีข้อเสียเยอะไปหน่อย ห้องเล็กเกินไป แอร์เสียงดังรบกวนเวลานอน ประตูห้องน้ำปิดไม่สนิท แล้วก็มีมดเยอะมาก ข้อดีคือใกล้ตลาด เดินไปซื้อของกินได้สะดวก แต่ภาพรวมยังไม่คุ้มค่าเท่าไหร่ ถ้ามีทางเลือกอื่นก็น่าจะดีกว่านะ\", 2],\n",
    "    [\"ชอบหอนี้มากค่ะ ห้องกว้างสะอาด เฟอร์นิเจอร์ครบครัน แอร์เย็นฉ่ำ มีโต๊ะเครื่องแป้งด้วย สะดวกมาก อินเทอร์เน็ตเร็ว เล่นเกมสบาย เจ้าของหอใจดี มีอะไรแจ้งปุ๊บมาดูปั๊บ ข้อเสียเล็กๆคือค่าไฟค่อนข้างแพง แล้วก็ซักผ้าต้องลงไปชั้นล่าง อยากให้มีเครื่องซักผ้าทุกชั้น แต่โดยรวมพอใจมากค่ะ แนะนำเลย\", 4],\n",
    "    [\"Terrible hotel experience! The room was dirty, smelly, and completely different from the photos. Staff was rude and unhelpful. AC didn't work, hot water was cold, and WiFi was extremely slow. The bed was uncomfortable and the bathroom was disgusting. Would never stay here again. Complete waste of money!\", 1],\n",
    "    [\"The hotel is okay for the price. Room was decent size but could be cleaner. Staff was friendly but service was slow. Location is good, close to attractions. Some facilities need maintenance. WiFi worked well. Overall, it's an average hotel - nothing special but acceptable for a short stay.\", 3],\n",
    "    [\"Amazing hotel! Absolutely loved our stay here. The room was spacious, clean, and beautifully decorated. Staff was incredibly friendly and helpful. The location is perfect - walking distance to everything. Breakfast was delicious with great variety. Pool and gym facilities were excellent. Highly recommend this place!\", 5],\n",
    "    [\"This hotel is really good! ห้องสะอาดมาก แอร์เย็น wifi super fast แต่ราคาแพงไปหน่อย but overall worth it นะ staff friendly มาก highly recommended! 👍\", 4],\n",
    "    [\"หอนี้ดีที่สุดในย่านนี้แล้วว อยู่มา 3 ปีไม่เคยมีปัญหาเลย ห้องกว้าง สะอาด ตกแต่งสวย มีเฟอร์ครบ เหมือนอยู่คอนโด เน็ตไวมาก 100 Mbps เล่นเกมไม่มีสะดุด! ระบบรักษาความปลอดภัยแน่นมาก มีกล้องวงจรปิด คีย์การ์ดทุกชั้น และมี รปภ. 24 ชม. ทีเด็ดสุดคือมีฟิตเนสและสระว่ายน้ำให้ใช้ฟรี คุ้มมากกกก แนะนำสุดๆ ถ้าได้ห้องก็จองเลยอย่ารอ!\", 5]\n",
    "]\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"การทดสอบกับรีวิวจริง\")\n",
    "print(\"=\"*60)\n",
    "print(f\"🏆 ทดสอบกับโมเดลที่ดีที่สุด: {best_model_name}\")\n",
    "test_results = []\n",
    "for review, actual_rating in test_reviews:\n",
    "    predicted_rating, actual = print_prediction_results_enhanced(review, actual_rating, best_model_name)\n",
    "    test_results.append((predicted_rating, actual))\n",
    "\n",
    "correct_predictions = sum(1 for pred, actual in test_results if pred == actual)\n",
    "test_accuracy = correct_predictions / len(test_results)\n",
    "print(f\"\\n📈 ผลสรุปการทดสอบรีวิว (โมเดลที่ดีที่สุด: {best_model_name}):\")\n",
    "print(f\"ทำนายถูกต้อง: {correct_predictions} รีวิว\")\n",
    "print(f\"ทำนายผิดพลาด: {len(test_results) - correct_predictions} รีวิว\")\n",
    "print(f\"ความแม่นยำรวม: {test_accuracy:.2f} ({correct_predictions}/{len(test_results)})\")\n",
    "\n",
    "### **17. เปรียบเทียบโมเดลกับรีวิวทดสอบ**\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"เปรียบเทียบทุกโมเดลกับรีวิวทดสอบ\")\n",
    "print(\"=\"*60)\n",
    "model_test_results = {}\n",
    "for model_name in results.keys():\n",
    "    print(f\"\\n🔍 ทดสอบ {model_name}:\")\n",
    "    model_results = []\n",
    "    for idx, (review, actual_rating) in enumerate(test_reviews[:5]):\n",
    "        predicted_rating, _ = predict_review_enhanced(review, model_name)\n",
    "        model_results.append(predicted_rating == actual_rating)\n",
    "        print(f\"  รีวิวที่ {idx+1}: {'✅' if model_results[-1] else '❌'}\")\n",
    "    accuracy_small = sum(model_results) / len(model_results)\n",
    "    model_test_results[model_name] = accuracy_small\n",
    "    print(f\"  ความแม่นยำในตัวอย่าง: {accuracy_small:.2f}\")\n",
    "\n",
    "### **18. การวิเคราะห์ Feature Importance (สำหรับ tree-based models)**\n",
    "if 'Random Forest' in best_models:\n",
    "    print(f\"\\n🌳 การวิเคราะห์ความสำคัญของ Features (Random Forest):\")\n",
    "    rf_model = best_models['Random Forest']\n",
    "    tfidf_features = [f\"tfidf_{i}\" for i in range(X_train_tfidf.shape[1])]\n",
    "    count_features = [f\"count_{i}\" for i in range(X_train_count.shape[1])]\n",
    "    embedding_features = [f\"embed_{i}\" for i in range(X_train_vectors.shape[1])]\n",
    "    all_feature_names = tfidf_features + count_features + embedding_features + feature_names\n",
    "    feature_importance = rf_model.feature_importances_\n",
    "    top_k = min(20, feature_importance.shape[0])\n",
    "    top_indices = np.argsort(feature_importance)[-top_k:]\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    plt.barh(range(top_k), feature_importance[top_indices])\n",
    "    plt.yticks(range(top_k), [all_feature_names[i] for i in top_indices])\n",
    "    plt.xlabel('ความสำคัญของ Feature'); plt.title(f'{top_k} Features ที่สำคัญที่สุด (Random Forest)')\n",
    "    plt.tight_layout(); plt.savefig(\"feature_importance.png\", dpi=300, bbox_inches='tight'); plt.show()\n",
    "\n",
    "### **19. สรุปประสิทธิภาพ**\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"สรุปประสิทธิภาพสุดท้าย\")\n",
    "print(\"=\"*60)\n",
    "summary_df = pd.DataFrame({\n",
    "    'Model': results_df['Model'],\n",
    "    'Test Accuracy': results_df['Test Accuracy'],\n",
    "    'CV Accuracy': results_df['CV Mean'],\n",
    "    'Std Dev': results_df['CV Std'],\n",
    "    'Training Time': results_df['Training Time (s)'],\n",
    "    'Rank': results_df['Test Accuracy'].rank(ascending=False).astype(int)\n",
    "}).sort_values('Test Accuracy', ascending=False)\n",
    "print(summary_df.round(4))\n",
    "\n",
    "print(f\"\\n🎯 ข้อมูลเชิงลึกสำคัญ:\")\n",
    "print(f\"• โมเดลที่ดีที่สุด: {best_model_name} (ความแม่นยำ: {results[best_model_name]['accuracy']:.4f})\")\n",
    "print(f\"• การใช้ FastText + Thai2Vec embeddings ช่วยจัดการภาษาผสมได้ดีขึ้น\")\n",
    "print(f\"• โมเดลสามารถจัดการคำภาษาอังกฤษในรีวิวไทยได้แล้ว\")\n",
    "print(f\"• Features ทั้งหมด: {X_train_combined.shape[1]:,}\")\n",
    "print(f\"• รองรับทั้งรีวิวภาษาไทยและอังกฤษ\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
