{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d959e63f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to C:\\Users\\Phutawan\n",
      "[nltk_data]     Chonsakorn\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to C:\\Users\\Phutawan\n",
      "[nltk_data]     Chonsakorn\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to C:\\Users\\Phutawan\n",
      "[nltk_data]     Chonsakorn\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‡∏Å‡∏≥‡∏•‡∏±‡∏á‡πÇ‡∏´‡∏•‡∏î Thai2Vec model...\n",
      "‡∏Å‡∏≥‡∏•‡∏±‡∏á‡πÇ‡∏´‡∏•‡∏î fastText model...\n",
      "‡πÇ‡∏´‡∏•‡∏î FastText model ‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à\n",
      "‡∏Å‡∏≥‡∏•‡∏±‡∏á‡∏ó‡∏≥‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏∞‡∏≠‡∏≤‡∏î‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•...\n",
      "‡∏Å‡∏≥‡∏•‡∏±‡∏á‡∏™‡∏Å‡∏±‡∏î‡∏Ñ‡∏∏‡∏ì‡∏•‡∏±‡∏Å‡∏©‡∏ì‡∏∞‡∏à‡∏≤‡∏Å‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°...\n",
      "‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏´‡∏•‡∏±‡∏á‡∏ó‡∏≥‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏∞‡∏≠‡∏≤‡∏î: 22493\n",
      "‡∏Å‡∏≤‡∏£‡∏Å‡∏£‡∏∞‡∏à‡∏≤‡∏¢‡∏Ç‡∏≠‡∏á‡∏Ñ‡∏∞‡πÅ‡∏ô‡∏ô‡∏´‡∏•‡∏±‡∏á‡∏ó‡∏≥‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏∞‡∏≠‡∏≤‡∏î:\n",
      "rating\n",
      "1    1821\n",
      "2    2191\n",
      "3    2584\n",
      "4    6448\n",
      "5    9449\n",
      "Name: count, dtype: int64\n",
      "\n",
      "‡∏Å‡∏≤‡∏£‡∏Å‡∏£‡∏∞‡∏à‡∏≤‡∏¢‡∏ï‡∏≤‡∏°‡∏†‡∏≤‡∏©‡∏≤‡∏´‡∏•‡∏±‡∏á‡∏ó‡∏≥‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏∞‡∏≠‡∏≤‡∏î:\n",
      "language\n",
      "english    20491\n",
      "thai        2002\n",
      "Name: count, dtype: int64\n",
      "‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ù‡∏∂‡∏Å‡∏ù‡∏ô: 17994\n",
      "‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡∏î‡∏™‡∏≠‡∏ö: 4499\n",
      "‡∏Å‡∏≥‡∏•‡∏±‡∏á‡∏™‡∏£‡πâ‡∏≤‡∏á word embeddings...\n",
      "‡∏Å‡∏≥‡∏•‡∏±‡∏á‡∏™‡∏£‡πâ‡∏≤‡∏á TF-IDF features...\n",
      "‡∏Å‡∏≥‡∏•‡∏±‡∏á‡∏™‡∏£‡πâ‡∏≤‡∏á Count vectors...\n",
      "‡∏Å‡∏≥‡∏•‡∏±‡∏á scale vectors (embeddings)...\n",
      "TF-IDF features: 8000\n",
      "Count features: 2000\n",
      "Word embedding features: 300\n",
      "Additional features: 11\n",
      "‡∏Å‡∏≥‡∏•‡∏±‡∏á‡∏£‡∏ß‡∏° features ‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î...\n",
      "‡∏Ç‡∏ô‡∏≤‡∏î‡∏Ç‡∏≠‡∏á features ‡∏£‡∏ß‡∏° - train: (17994, 10311), test: (4499, 10311)\n",
      "‡πÅ‡∏õ‡∏•‡∏á dense ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏ö‡∏≤‡∏á‡πÇ‡∏°‡πÄ‡∏î‡∏•...\n",
      "\n",
      "==================================================\n",
      "‡∏Å‡∏≥‡∏•‡∏±‡∏á‡∏ù‡∏∂‡∏Å‡∏ù‡∏ô‡πÅ‡∏•‡∏∞‡∏õ‡∏£‡∏∞‡πÄ‡∏°‡∏¥‡∏ô‡∏ú‡∏• MODELS\n",
      "==================================================\n",
      "\n",
      "üîß ‡∏Å‡∏≥‡∏•‡∏±‡∏á‡∏ù‡∏∂‡∏Å‡∏ù‡∏ô Logistic Regression...\n",
      "‚úÖ Logistic Regression ‡πÄ‡∏™‡∏£‡πá‡∏à‡∏™‡∏¥‡πâ‡∏ô!\n",
      "   ‡∏Ñ‡∏ß‡∏≤‡∏°‡πÅ‡∏°‡πà‡∏ô‡∏¢‡∏≥‡∏Å‡∏≤‡∏£‡∏ó‡∏î‡∏™‡∏≠‡∏ö: 0.6026\n",
      "   ‡∏Ñ‡∏ß‡∏≤‡∏°‡πÅ‡∏°‡πà‡∏ô‡∏¢‡∏≥ CV: 0.5979 (¬±0.0126)\n",
      "   ‡πÄ‡∏ß‡∏•‡∏≤‡∏ù‡∏∂‡∏Å‡∏ù‡∏ô: 183.43 ‡∏ß‡∏¥‡∏ô‡∏≤‡∏ó‡∏µ\n",
      "\n",
      "üîß ‡∏Å‡∏≥‡∏•‡∏±‡∏á‡∏ù‡∏∂‡∏Å‡∏ù‡∏ô Random Forest...\n",
      "‚úÖ Random Forest ‡πÄ‡∏™‡∏£‡πá‡∏à‡∏™‡∏¥‡πâ‡∏ô!\n",
      "   ‡∏Ñ‡∏ß‡∏≤‡∏°‡πÅ‡∏°‡πà‡∏ô‡∏¢‡∏≥‡∏Å‡∏≤‡∏£‡∏ó‡∏î‡∏™‡∏≠‡∏ö: 0.5719\n",
      "   ‡∏Ñ‡∏ß‡∏≤‡∏°‡πÅ‡∏°‡πà‡∏ô‡∏¢‡∏≥ CV: 0.5730 (¬±0.0086)\n",
      "   ‡πÄ‡∏ß‡∏•‡∏≤‡∏ù‡∏∂‡∏Å‡∏ù‡∏ô: 32.08 ‡∏ß‡∏¥‡∏ô‡∏≤‡∏ó‡∏µ\n",
      "\n",
      "üîß ‡∏Å‡∏≥‡∏•‡∏±‡∏á‡∏ù‡∏∂‡∏Å‡∏ù‡∏ô Linear SVM...\n"
     ]
    }
   ],
   "source": [
    "### **0. Install Required Libraries**\n",
    "# !pip install numpy==1.23.5\n",
    "# !pip install --upgrade gensim\n",
    "# !pip install --upgrade pythainlp\n",
    "# !pip install emoji\n",
    "# !pip install fasttext-wheel  # ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö fastText embeddings\n",
    "\n",
    "### **1. Import Libraries**\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import time\n",
    "import re\n",
    "import string\n",
    "import emoji\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Thai NLP - ‡πÄ‡∏Ñ‡∏£‡∏∑‡πà‡∏≠‡∏á‡∏°‡∏∑‡∏≠‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏•‡∏†‡∏≤‡∏©‡∏≤‡πÑ‡∏ó‡∏¢\n",
    "from pythainlp.tokenize import word_tokenize\n",
    "from pythainlp.corpus import thai_negations\n",
    "from pythainlp.word_vector import WordVector\n",
    "\n",
    "# Machine Learning - ‡πÄ‡∏Ñ‡∏£‡∏∑‡πà‡∏≠‡∏á‡∏°‡∏∑‡∏≠‡πÅ‡∏°‡∏ä‡∏ä‡∏µ‡∏ô‡πÄ‡∏•‡∏¥‡∏£‡πå‡∏ô‡∏ô‡∏¥‡∏á\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "from scipy.sparse import hstack, csr_matrix\n",
    "from collections import Counter\n",
    "\n",
    "# FastText - ‡πÄ‡∏Ñ‡∏£‡∏∑‡πà‡∏≠‡∏á‡∏°‡∏∑‡∏≠‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö word embedding\n",
    "import fasttext\n",
    "import fasttext.util\n",
    "\n",
    "# NLTK ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏†‡∏≤‡∏©‡∏≤‡∏≠‡∏±‡∏á‡∏Å‡∏§‡∏©\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize as nltk_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "# ‡∏î‡∏≤‡∏ß‡∏ô‡πå‡πÇ‡∏´‡∏•‡∏î NLTK data (‡∏£‡∏±‡∏ô‡∏Ñ‡∏£‡∏±‡πâ‡∏á‡πÅ‡∏£‡∏Å‡πÄ‡∏ó‡πà‡∏≤‡∏ô‡∏±‡πâ‡∏ô)\n",
    "try:\n",
    "    nltk.data.find('tokenizers/punkt')\n",
    "    nltk.data.find('corpora/stopwords')\n",
    "    nltk.data.find('corpora/wordnet')\n",
    "except LookupError:\n",
    "    nltk.download('punkt')\n",
    "    nltk.download('stopwords')\n",
    "    nltk.download('wordnet')\n",
    "\n",
    "## **2. Load and Explore Data**\n",
    "# ‡∏Å‡∏≥‡∏´‡∏ô‡∏î‡∏Å‡∏≤‡∏£‡πÅ‡∏™‡∏î‡∏á‡∏ú‡∏•‡∏†‡∏≤‡∏©‡∏≤‡πÑ‡∏ó‡∏¢\n",
    "import matplotlib\n",
    "matplotlib.rcParams['font.family'] = 'DejaVu Sans'\n",
    "\n",
    "# ‡πÇ‡∏´‡∏•‡∏î dataset ‡∏†‡∏≤‡∏©‡∏≤‡πÑ‡∏ó‡∏¢ (‡∏£‡∏µ‡∏ß‡∏¥‡∏ß‡∏´‡∏≠‡∏û‡∏±‡∏Å)\n",
    "df_thai = pd.read_csv(\"Data/dorm_reviews.csv\")\n",
    "# ‡∏Ñ‡∏≤‡∏î‡∏ß‡πà‡∏≤‡πÑ‡∏ü‡∏•‡πå‡πÑ‡∏ó‡∏¢‡∏°‡∏µ‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå text, rating ‡∏≠‡∏¢‡∏π‡πà‡πÅ‡∏•‡πâ‡∏ß\n",
    "if 'dormitory_id' in df_thai.columns:\n",
    "    df_thai = df_thai.drop([c for c in ['dormitory_id', 'user_id'] if c in df_thai.columns], axis=1)\n",
    "df_thai = df_thai.rename(columns={'Review':'text','Rating':'rating'})\n",
    "df_thai = df_thai[['text','rating']].copy()\n",
    "df_thai['language'] = 'thai'\n",
    "\n",
    "# ‡πÇ‡∏´‡∏•‡∏î dataset ‡∏†‡∏≤‡∏©‡∏≤‡∏≠‡∏±‡∏á‡∏Å‡∏§‡∏© (‡∏£‡∏µ‡∏ß‡∏¥‡∏ß‡πÇ‡∏£‡∏á‡πÅ‡∏£‡∏°)\n",
    "df_english = pd.read_csv(\"Data/tripadvisor_hotel_reviews.csv\")\n",
    "# ‡πÉ‡∏´‡πâ‡πÄ‡∏´‡∏•‡∏∑‡∏≠‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå text, rating\n",
    "if {'Review','Rating'}.issubset(df_english.columns):\n",
    "    df_english = df_english[['Review','Rating']].copy()\n",
    "    df_english.columns = ['text','rating']\n",
    "else:\n",
    "    # ‡πÄ‡∏ú‡∏∑‡πà‡∏≠‡πÑ‡∏ü‡∏•‡πå‡πÉ‡∏ä‡πâ‡∏ä‡∏∑‡πà‡∏≠‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå‡∏≠‡∏∑‡πà‡∏ô\n",
    "    possible_text = [c for c in df_english.columns if c.lower() in ['review','text','comment','content']]\n",
    "    possible_rating = [c for c in df_english.columns if c.lower() in ['rating','score','stars']]\n",
    "    df_english = df_english[[possible_text[0], possible_rating[0]]].copy()\n",
    "    df_english.columns = ['text','rating']\n",
    "df_english['language'] = 'english'\n",
    "\n",
    "# ‡∏£‡∏ß‡∏°‡∏™‡∏≠‡∏á‡∏ä‡∏∏‡∏î‡πÄ‡∏õ‡πá‡∏ô df (‡πÅ‡∏Å‡πâ NameError ‡πÄ‡∏î‡∏¥‡∏°)\n",
    "df = pd.concat([df_thai, df_english], ignore_index=True)\n",
    "\n",
    "# ‡∏ó‡∏≥ rating ‡πÉ‡∏´‡πâ‡πÄ‡∏õ‡πá‡∏ô int 1..5\n",
    "df['rating'] = pd.to_numeric(df['rating'], errors='coerce').clip(1,5).astype(int)\n",
    "\n",
    "### **3. ‡πÇ‡∏´‡∏•‡∏î Word Embedding Models**\n",
    "# ‡πÇ‡∏´‡∏•‡∏î Thai2Vec model ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏†‡∏≤‡∏©‡∏≤‡πÑ‡∏ó‡∏¢\n",
    "print(\"‡∏Å‡∏≥‡∏•‡∏±‡∏á‡πÇ‡∏´‡∏•‡∏î Thai2Vec model...\")\n",
    "thai2fit_model = WordVector(model_name=\"thai2fit_wv\").get_model()\n",
    "\n",
    "# ‡∏î‡∏≤‡∏ß‡∏ô‡πå‡πÇ‡∏´‡∏•‡∏î‡πÅ‡∏•‡∏∞‡πÇ‡∏´‡∏•‡∏î fastText model ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏†‡∏≤‡∏©‡∏≤‡∏≠‡∏±‡∏á‡∏Å‡∏§‡∏©\n",
    "print(\"‡∏Å‡∏≥‡∏•‡∏±‡∏á‡πÇ‡∏´‡∏•‡∏î fastText model...\")\n",
    "fasttext_model = fasttext.load_model('cc.en.300.bin')\n",
    "print(\"‡πÇ‡∏´‡∏•‡∏î FastText model ‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à\")\n",
    "\n",
    "def enhanced_sentence_vectorizer(text, dim=300):\n",
    "    \"\"\"\n",
    "    ‡∏ü‡∏±‡∏á‡∏Å‡πå‡∏ä‡∏±‡∏ô‡πÅ‡∏õ‡∏•‡∏á‡∏õ‡∏£‡∏∞‡πÇ‡∏¢‡∏Ñ‡πÄ‡∏õ‡πá‡∏ô vector ‡πÇ‡∏î‡∏¢‡πÉ‡∏ä‡πâ Thai2Vec ‡πÅ‡∏•‡∏∞ fastText\n",
    "    \"\"\"\n",
    "    if not isinstance(text, str):\n",
    "        text = \"\"\n",
    "    is_thai = any(ord(char) >= 3584 and ord(char) <= 3711 for char in text)\n",
    "    words = word_tokenize(text, engine=\"newmm\") if is_thai else nltk_tokenize(text.lower())\n",
    "\n",
    "    vec = np.zeros(dim, dtype=np.float32)\n",
    "    word_count = 0\n",
    "    total_words = len(words)\n",
    "    \n",
    "    for i, word in enumerate(words):\n",
    "        if word.strip():\n",
    "            position_weight = 1.0 + (i / max(total_words, 1)) * 0.5\n",
    "            word_vector = None\n",
    "            \n",
    "            if is_thai:\n",
    "                if word in thai2fit_model:\n",
    "                    word_vector = thai2fit_model[word] * position_weight\n",
    "                else:\n",
    "                    char_vec, char_count = np.zeros(dim, dtype=np.float32), 0\n",
    "                    for char in word:\n",
    "                        if char in thai2fit_model:\n",
    "                            char_vec += thai2fit_model[char]\n",
    "                            char_count += 1\n",
    "                    if char_count > 0:\n",
    "                        word_vector = (char_vec / char_count) * position_weight\n",
    "            else:\n",
    "                try:\n",
    "                    word_vector = fasttext_model.get_word_vector(word) * position_weight\n",
    "                except Exception:\n",
    "                    pass\n",
    "            \n",
    "            if word_vector is not None:\n",
    "                vec += word_vector\n",
    "                word_count += 1\n",
    "    \n",
    "    return vec / max(word_count, 1)\n",
    "\n",
    "### **4. ‡∏ü‡∏±‡∏á‡∏Å‡πå‡∏ä‡∏±‡∏ô‡∏ó‡∏≥‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏∞‡∏≠‡∏≤‡∏î‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡πÅ‡∏•‡∏∞‡∏Å‡∏≤‡∏£‡∏™‡∏Å‡∏±‡∏î‡∏Ñ‡∏∏‡∏ì‡∏•‡∏±‡∏Å‡∏©‡∏ì‡∏∞**\n",
    "def clean_text_thai(text):\n",
    "    if not isinstance(text, str):\n",
    "        return \"\"\n",
    "    from pythainlp.corpus import thai_stopwords\n",
    "    thai_stop_words = list(thai_stopwords())\n",
    "    important_words = [\n",
    "        \"‡πÑ‡∏°‡πà\",\"‡πÑ‡∏°‡πà‡∏°‡∏µ\",\"‡πÑ‡∏°‡πà‡πÑ‡∏î‡πâ\",\"‡πÑ‡∏°‡πà‡∏Ñ‡πà‡∏≠‡∏¢\",\"‡πÑ‡∏°‡πà‡πÄ‡∏Ñ‡∏¢\",\"‡∏´‡πâ‡∏≤‡∏°\",\"‡∏¢‡∏±‡∏á‡πÑ‡∏°‡πà\",\"‡πÑ‡∏°‡πà‡∏¢‡∏≠‡∏°\",\n",
    "        \"‡∏î‡∏µ\",\"‡∏î‡∏µ‡∏°‡∏≤‡∏Å\",\"‡∏™‡∏∞‡∏≠‡∏≤‡∏î\",\"‡πÄ‡∏¢‡πá‡∏ô\",\"‡∏Å‡∏ß‡πâ‡∏≤‡∏á\",\"‡πÉ‡∏´‡∏°‡πà\",\"‡∏™‡∏ß‡∏¢\",\"‡∏ô‡πà‡∏≤‡∏≠‡∏¢‡∏π‡πà\",\"‡∏™‡∏ö‡∏≤‡∏¢\",\n",
    "        \"‡∏ä‡∏≠‡∏ö\",\"‡∏õ‡∏£‡∏∞‡∏ó‡∏±‡∏ö‡πÉ‡∏à\",\"‡∏™‡∏∞‡∏î‡∏ß‡∏Å\",\"‡∏õ‡∏•‡∏≠‡∏î‡∏†‡∏±‡∏¢\",\"‡∏Ñ‡∏∏‡πâ‡∏°\",\"‡∏Ñ‡∏∏‡πâ‡∏°‡∏Ñ‡πà‡∏≤\",\"‡πÄ‡∏á‡∏µ‡∏¢‡∏ö\",\"‡∏Ñ‡∏£‡∏ö\",\n",
    "        \"‡∏û‡∏≠‡πÉ‡∏à\",\"‡πÄ‡∏£‡πá‡∏ß\",\"‡πÇ‡∏≠‡πÄ‡∏Ñ\",\"‡πÇ‡∏≠‡πÄ‡∏Ñ‡πÄ‡∏•‡∏¢\",\"‡πÄ‡∏¢‡∏µ‡πà‡∏¢‡∏°\",\"‡∏ñ‡∏π‡∏Å‡πÉ‡∏à\",\"‡∏ó‡∏≥‡πÄ‡∏•‡∏î‡∏µ\",\"‡πÉ‡∏Å‡∏•‡πâ\",\"‡∏Ñ‡∏£‡∏ö‡∏Ñ‡∏£‡∏±‡∏ô\",\n",
    "        \"‡πÅ‡∏¢‡πà\",\"‡πÑ‡∏°‡πà‡∏î‡∏µ\",\"‡πÄ‡∏´‡∏°‡πá‡∏ô\",\"‡∏£‡πâ‡∏≠‡∏ô\",\"‡πÅ‡∏Ñ‡∏ö\",\"‡πÄ‡∏Å‡πà‡∏≤\",\"‡∏™‡∏Å‡∏õ‡∏£‡∏Å\",\"‡∏û‡∏±‡∏á\",\"‡πÄ‡∏™‡∏µ‡∏¢‡∏á‡∏î‡∏±‡∏á\",\n",
    "        \"‡πÅ‡∏û‡∏á\",\"‡πÑ‡∏°‡πà‡∏ä‡∏≠‡∏ö\",\"‡πÅ‡∏≠‡∏≠‡∏±‡∏î\",\"‡∏£‡∏Å\",\"‡∏≠‡∏±‡∏ô‡∏ï‡∏£‡∏≤‡∏¢\",\"‡∏ä‡πâ‡∏≤\",\"‡∏ú‡∏¥‡∏î‡∏´‡∏ß‡∏±‡∏á\",\"‡∏´‡πà‡∏ß‡∏¢\",\"‡πÄ‡∏ü‡∏•\",\n",
    "        \"‡∏Å‡∏≤‡∏Å\",\"‡πÑ‡∏°‡πà‡∏Ñ‡∏∏‡πâ‡∏°\",\"‡πÑ‡∏Å‡∏•\",\"‡∏£‡∏±‡πà‡∏ß\",\"‡∏ó‡∏£‡∏∏‡∏î\",\"‡∏ó‡∏£‡∏∏‡∏î‡πÇ‡∏ó‡∏£‡∏°\",\"‡πÄ‡∏™‡∏∑‡πà‡∏≠‡∏°\",\n",
    "        \"‡∏°‡∏≤‡∏Å\",\"‡∏™‡∏∏‡∏î‡πÜ\",\"‡πÄ‡∏¢‡∏≠‡∏∞\",\"‡∏ô‡πâ‡∏≠‡∏¢\",\"‡∏ó‡∏µ‡πà‡∏™‡∏∏‡∏î\",\"‡∏´‡∏•‡∏≤‡∏¢\",\"‡∏ó‡∏∏‡∏Å\",\"‡πÄ‡∏Å‡∏¥‡∏ô\",\"‡∏à‡∏±‡∏î\",\"‡πÇ‡∏Ñ‡∏ï‡∏£\",\n",
    "        \"‡∏°‡∏≤‡∏Å‡πÜ\",\"‡∏™‡∏∏‡∏î‡∏¢‡∏≠‡∏î\",\"‡∏ò‡∏£‡∏£‡∏°‡∏î‡∏≤\",\"‡∏û‡∏≠‡πÉ‡∏ä‡πâ\",\"‡∏ö‡πà‡∏≠‡∏¢\",\"‡∏ï‡∏•‡∏≠‡∏î\",\"‡πÄ‡∏ß‡∏≠‡∏£‡πå\",\n",
    "        \"‡πÅ‡∏≠‡∏£‡πå\",\"‡∏ô‡πâ‡∏≥\",\"‡πÑ‡∏ü\",\"‡∏´‡πâ‡∏≠‡∏á‡∏ô‡πâ‡∏≥\",\"‡πÄ‡∏ï‡∏µ‡∏¢‡∏á\",\"‡∏ù‡∏±‡∏Å‡∏ö‡∏±‡∏ß\",\"‡πÄ‡∏ô‡πá‡∏ï\",\"‡πÑ‡∏ß‡πÑ‡∏ü\",\"‡πÑ‡∏ü‡∏ü‡πâ‡∏≤\",\n",
    "        \"‡∏õ‡∏£‡∏∞‡∏õ‡∏≤\",\"‡πÄ‡∏ü‡∏≠‡∏£‡πå\",\"‡∏•‡∏¥‡∏ü‡∏ï‡πå\",\"‡∏ó‡∏µ‡πà‡∏à‡∏≠‡∏î\",\"‡∏à‡∏≠‡∏î‡∏£‡∏ñ\",\"‡∏ã‡∏±‡∏Å‡∏ú‡πâ‡∏≤\",\"‡∏ï‡∏π‡πâ‡πÄ‡∏¢‡πá‡∏ô\",\"‡∏ó‡∏µ‡∏ß‡∏µ\",\n",
    "        \"‡∏à‡∏≤‡∏ô\",\"‡πÑ‡∏°‡πÇ‡∏Ñ‡∏£‡πÄ‡∏ß‡∏ü\",\"‡πÄ‡∏ï‡∏≤\",\"‡∏ô‡πâ‡∏≥‡∏≠‡∏∏‡πà‡∏ô\",\"‡∏ú‡πâ‡∏≤‡∏õ‡∏π\",\"‡πÇ‡∏ï‡πä‡∏∞\",\"‡πÄ‡∏Å‡πâ‡∏≤‡∏≠‡∏µ‡πâ\",\"‡∏ï‡∏π‡πâ\",\"‡∏ä‡∏±‡πâ‡∏ô‡∏ß‡∏≤‡∏á\",\n",
    "        \"‡∏õ‡∏•‡∏±‡πä‡∏Å\",\"‡∏™‡∏±‡∏ç‡∏ç‡∏≤‡∏ì\",\n",
    "        \"‡πÄ‡∏™‡∏µ‡∏¢‡∏á\",\"‡∏°‡∏î\",\"‡πÅ‡∏°‡∏•‡∏á\",\"‡πÅ‡∏°‡∏•‡∏á‡∏™‡∏≤‡∏ö\",\"‡∏´‡∏ô‡∏π\",\"‡∏¢‡∏∏‡∏á\",\"‡∏ù‡∏∏‡πà‡∏ô\",\"‡∏Å‡∏•‡∏¥‡πà‡∏ô\",\"‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏ô‡∏ö‡πâ‡∏≤‡∏ô\",\n",
    "        \"‡∏Ç‡πâ‡∏≤‡∏á‡∏´‡πâ‡∏≠‡∏á\",\"‡∏Ç‡πâ‡∏≤‡∏á‡∏ô‡∏≠‡∏Å\",\"‡∏ñ‡∏ô‡∏ô\",\"‡∏ó‡∏≤‡∏á‡πÄ‡∏î‡∏¥‡∏ô\",\"‡∏•‡∏≤‡∏ô‡∏à‡∏≠‡∏î\",\"‡∏ä‡∏±‡πâ‡∏ô‡∏ö‡∏ô\",\"‡∏ö‡∏±‡∏ô‡πÑ‡∏î\",\"‡∏Å‡∏≥‡πÅ‡∏û‡∏á\",\n",
    "        \"‡∏î‡∏π‡πÅ‡∏•\",\"‡∏ö‡∏£‡∏¥‡∏Å‡∏≤‡∏£\",\"‡∏ã‡πà‡∏≠‡∏°\",\"‡πÅ‡∏Å‡πâ‡πÑ‡∏Ç\",\"‡∏à‡∏±‡∏î‡∏Å‡∏≤‡∏£\",\"‡∏û‡∏ô‡∏±‡∏Å‡∏á‡∏≤‡∏ô\",\"‡πÅ‡∏°‡πà‡∏ö‡πâ‡∏≤‡∏ô\",\"‡∏£‡∏õ‡∏†\",\n",
    "        \"‡πÄ‡∏à‡πâ‡∏≤‡∏Ç‡∏≠‡∏á\",\"‡∏ô‡∏¥‡∏ï‡∏¥\",\"‡∏Å‡∏é\",\"‡∏£‡∏∞‡πÄ‡∏ö‡∏µ‡∏¢‡∏ö\",\"‡∏Ñ‡πà‡∏≤‡πÄ‡∏ä‡πà‡∏≤\",\"‡∏Ñ‡πà‡∏≤‡πÑ‡∏ü\",\"‡∏Ñ‡πà‡∏≤‡∏ô‡πâ‡∏≥\",\"‡∏Ñ‡πà‡∏≤‡∏™‡πà‡∏ß‡∏ô‡∏Å‡∏•‡∏≤‡∏á\",\n",
    "        \"‡∏°‡∏±‡∏î‡∏à‡∏≥\",\"‡∏õ‡∏£‡∏∞‡∏Å‡∏±‡∏ô\",\"‡∏™‡∏±‡∏ç‡∏ç‡∏≤\",\"‡∏ù‡∏≤‡∏Å‡∏Ç‡∏≠‡∏á\",\"‡∏£‡∏±‡∏ö‡∏û‡∏±‡∏™‡∏î‡∏∏\",\"‡∏Ñ‡∏µ‡∏¢‡πå‡∏Å‡∏≤‡∏£‡πå‡∏î\",\"‡∏•‡πá‡∏≠‡∏Ñ\",\"‡∏£‡∏≠‡∏ô‡∏≤‡∏ô\",\n",
    "        \"‡πÑ‡∏°‡πà‡∏°‡∏≤‡∏î‡∏π\",\"‡πÑ‡∏°‡πà‡∏ã‡πà‡∏≠‡∏°\",\n",
    "        \"‡πÅ‡∏ï‡πà\",\"‡πÅ‡∏ï‡πà‡∏ß‡πà‡∏≤\",\"‡∏ñ‡∏∂‡∏á‡πÅ‡∏°‡πâ\",\"‡∏≠‡∏¢‡πà‡∏≤‡∏á‡πÑ‡∏£‡∏Å‡πá‡∏ï‡∏≤‡∏°\",\"‡πÄ‡∏û‡∏£‡∏≤‡∏∞\",\"‡πÄ‡∏û‡∏£‡∏≤‡∏∞‡∏ß‡πà‡∏≤\",\"‡πÄ‡∏ô‡∏∑‡πà‡∏≠‡∏á‡∏à‡∏≤‡∏Å\",\n",
    "        \"‡∏Ñ‡∏∑‡∏≠\",\"‡∏Å‡πá‡∏Ñ‡∏∑‡∏≠\",\"‡∏™‡πà‡∏ß‡∏ô\",\"‡∏ô‡∏≠‡∏Å‡∏à‡∏≤‡∏Å‡∏ô‡∏µ‡πâ\",\"‡∏ó‡∏µ‡πà‡∏à‡∏£‡∏¥‡∏á\",\"‡∏à‡∏£‡∏¥‡∏á‡πÜ\",\"‡∏Å‡πá\",\"‡πÅ‡∏°‡πâ\",\"‡∏ó‡∏µ‡πà\",\n",
    "        \"‡∏ï‡∏≠‡∏ô‡πÅ‡∏£‡∏Å\",\"‡∏û‡∏≠‡∏î‡∏µ\",\"‡πÅ‡∏•‡πâ‡∏ß‡∏Å‡πá\"\n",
    "    ]\n",
    "    custom_stop_words = [w for w in thai_stop_words if w not in important_words]\n",
    "    punct = string.punctuation.replace('!', '').replace('?', '').replace('.', '')\n",
    "    text = emoji.replace_emoji(text, replace=\"\")\n",
    "    text = ''.join(ch if ch not in punct else ' ' for ch in text)\n",
    "    text = re.sub(r'([‡∏Å-‡πôa-zA-Z])\\1{2,}', r'\\1\\1', text)\n",
    "    text = re.sub(r'\\s+', \" \", text).strip().lower()\n",
    "    words = word_tokenize(text, engine='newmm')\n",
    "    words = [w for w in words if w not in custom_stop_words]\n",
    "    return ' '.join(words)\n",
    "\n",
    "def clean_text_english(text):\n",
    "    if not isinstance(text, str):\n",
    "        return \"\"\n",
    "    english_stop_words = set(stopwords.words('english'))\n",
    "    important_english_words = {\n",
    "        'not','no','never','nothing','nowhere','neither','nobody','none',\n",
    "        'good','great','excellent','amazing','wonderful','perfect','love',\n",
    "        'best','nice','clean','comfortable','convenient','recommend',\n",
    "        'bad','terrible','awful','horrible','worst','hate','dirty',\n",
    "        'uncomfortable','expensive','cheap','noisy','small',\n",
    "        'very','really','extremely','quite','pretty','too','so','absolutely'\n",
    "    }\n",
    "    custom_stop_words = english_stop_words - important_english_words\n",
    "    punct = string.punctuation.replace('!', '').replace('?', '').replace('.', '')\n",
    "    text = emoji.replace_emoji(text, replace=\"\")\n",
    "    text = ''.join(ch if ch not in punct else ' ' for ch in text)\n",
    "    text = re.sub(r'([a-zA-Z])\\1{2,}', r'\\1\\1', text)\n",
    "    text = re.sub(r'\\s+', \" \", text).strip().lower()\n",
    "    words = nltk_tokenize(text)\n",
    "    lem = WordNetLemmatizer()\n",
    "    words = [lem.lemmatize(w) for w in words if w not in custom_stop_words and w.isalpha()]\n",
    "    return ' '.join(words)\n",
    "\n",
    "def clean_text(text):\n",
    "    is_thai = any(ord(c) >= 3584 and ord(c) <= 3711 for c in str(text))\n",
    "    return clean_text_thai(text) if is_thai else clean_text_english(text)\n",
    "\n",
    "def extract_features(text):\n",
    "    is_thai = any(ord(c) >= 3584 and ord(c) <= 3711 for c in str(text))\n",
    "    words = word_tokenize(text, engine='newmm') if is_thai else nltk_tokenize(text.lower())\n",
    "    word_count = len(words)\n",
    "    features = {\n",
    "        'exclamation_count': text.count('!'),\n",
    "        'question_count': text.count('?'),\n",
    "        'sentence_count': text.count('.') + 1,\n",
    "        'word_count': word_count,\n",
    "        'avg_word_length': sum(len(w) for w in words) / max(word_count, 1),\n",
    "        'text_length': len(text)\n",
    "    }\n",
    "    word_counts = Counter(words)\n",
    "    repeated_words = sum(1 for cnt in word_counts.values() if cnt > 1)\n",
    "    features['repeated_words_ratio'] = repeated_words / max(word_count, 1)\n",
    "    if is_thai:\n",
    "        negation_words = set(thai_negations())\n",
    "        features['negation_count'] = sum(1 for w in words if w in negation_words)\n",
    "    else:\n",
    "        english_negations = {'not','no','never','nothing','nobody','none','neither','nowhere'}\n",
    "        features['negation_count'] = sum(1 for w in words if w in english_negations)\n",
    "    features['punctuation_ratio'] = len([c for c in text if c in string.punctuation]) / max(len(text), 1)\n",
    "    features['words_per_sentence'] = word_count / max(features['sentence_count'], 1)\n",
    "    english_words = sum(1 for w in words if w.isascii() and w.isalpha())\n",
    "    features['english_ratio'] = english_words / max(word_count, 1)\n",
    "    return features\n",
    "\n",
    "### **5. ‡∏ó‡∏≥‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏∞‡∏≠‡∏≤‡∏î‡πÅ‡∏•‡∏∞‡πÄ‡∏ï‡∏£‡∏µ‡∏¢‡∏°‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•**\n",
    "print(\"‡∏Å‡∏≥‡∏•‡∏±‡∏á‡∏ó‡∏≥‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏∞‡∏≠‡∏≤‡∏î‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•...\")\n",
    "df['cleaned_review'] = df['text'].astype(str).apply(clean_text)\n",
    "\n",
    "def get_word_count(text):\n",
    "    is_thai = any(ord(char) >= 3584 and ord(char) <= 3711 for char in str(text))\n",
    "    return len(word_tokenize(text, engine='newmm')) if is_thai else len(nltk_tokenize(text))\n",
    "\n",
    "df = df[df['cleaned_review'].apply(get_word_count) > 3]\n",
    "df = df.drop_duplicates(subset=['cleaned_review'])\n",
    "\n",
    "print(\"‡∏Å‡∏≥‡∏•‡∏±‡∏á‡∏™‡∏Å‡∏±‡∏î‡∏Ñ‡∏∏‡∏ì‡∏•‡∏±‡∏Å‡∏©‡∏ì‡∏∞‡∏à‡∏≤‡∏Å‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°...\")\n",
    "feature_columns = ['cleaned_review']\n",
    "feature_names = ['exclamation_count','question_count','sentence_count','word_count',\n",
    "                 'avg_word_length','repeated_words_ratio','negation_count',\n",
    "                 'punctuation_ratio','text_length','words_per_sentence','english_ratio']\n",
    "\n",
    "for f in feature_names:\n",
    "    df[f] = df['cleaned_review'].apply(lambda x: extract_features(x)[f])\n",
    "\n",
    "feature_columns.extend(feature_names)\n",
    "\n",
    "print(f\"‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏´‡∏•‡∏±‡∏á‡∏ó‡∏≥‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏∞‡∏≠‡∏≤‡∏î: {len(df)}\")\n",
    "print(\"‡∏Å‡∏≤‡∏£‡∏Å‡∏£‡∏∞‡∏à‡∏≤‡∏¢‡∏Ç‡∏≠‡∏á‡∏Ñ‡∏∞‡πÅ‡∏ô‡∏ô‡∏´‡∏•‡∏±‡∏á‡∏ó‡∏≥‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏∞‡∏≠‡∏≤‡∏î:\")\n",
    "print(df['rating'].value_counts().sort_index())\n",
    "print(\"\\n‡∏Å‡∏≤‡∏£‡∏Å‡∏£‡∏∞‡∏à‡∏≤‡∏¢‡∏ï‡∏≤‡∏°‡∏†‡∏≤‡∏©‡∏≤‡∏´‡∏•‡∏±‡∏á‡∏ó‡∏≥‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏∞‡∏≠‡∏≤‡∏î:\")\n",
    "print(df['language'].value_counts())\n",
    "\n",
    "### **6. ‡πÅ‡∏ö‡πà‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•**\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    df[feature_columns], df['rating'],\n",
    "    test_size=0.2, random_state=42, stratify=df['rating'] if df['rating'].nunique()>1 else None\n",
    ")\n",
    "print(f\"‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ù‡∏∂‡∏Å‡∏ù‡∏ô: {len(X_train)}\")\n",
    "print(f\"‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡∏î‡∏™‡∏≠‡∏ö: {len(X_test)}\")\n",
    "\n",
    "### **7. ‡∏™‡∏£‡πâ‡∏≤‡∏á Feature Vectors**\n",
    "def custom_tokenizer_mixed(text):\n",
    "    is_thai = any(ord(char) >= 3584 and ord(char) <= 3711 for char in str(text))\n",
    "    return word_tokenize(text, engine='newmm') if is_thai else nltk_tokenize(text.lower())\n",
    "\n",
    "print(\"‡∏Å‡∏≥‡∏•‡∏±‡∏á‡∏™‡∏£‡πâ‡∏≤‡∏á word embeddings...\")\n",
    "X_train_vectors = np.array([enhanced_sentence_vectorizer(t) for t in X_train['cleaned_review']], dtype=np.float32)\n",
    "X_test_vectors  = np.array([enhanced_sentence_vectorizer(t) for t in X_test['cleaned_review']], dtype=np.float32)\n",
    "\n",
    "print(\"‡∏Å‡∏≥‡∏•‡∏±‡∏á‡∏™‡∏£‡πâ‡∏≤‡∏á TF-IDF features...\")\n",
    "tfidf_vectorizer = TfidfVectorizer(\n",
    "    tokenizer=custom_tokenizer_mixed,\n",
    "    max_features=8000,\n",
    "    ngram_range=(1,3),\n",
    "    min_df=2,\n",
    "    max_df=0.85,\n",
    "    use_idf=True,\n",
    "    smooth_idf=True,\n",
    "    sublinear_tf=True\n",
    ")\n",
    "\n",
    "print(\"‡∏Å‡∏≥‡∏•‡∏±‡∏á‡∏™‡∏£‡πâ‡∏≤‡∏á Count vectors...\")\n",
    "count_vectorizer = CountVectorizer(\n",
    "    tokenizer=custom_tokenizer_mixed,\n",
    "    max_features=2000,\n",
    "    ngram_range=(1,2),\n",
    "    min_df=3,\n",
    "    max_df=0.85\n",
    ")\n",
    "\n",
    "X_train_tfidf = tfidf_vectorizer.fit_transform(X_train['cleaned_review'])\n",
    "X_test_tfidf  = tfidf_vectorizer.transform(X_test['cleaned_review'])\n",
    "X_train_count = count_vectorizer.fit_transform(X_train['cleaned_review'])\n",
    "X_test_count  = count_vectorizer.transform(X_test['cleaned_review'])\n",
    "\n",
    "print(\"‡∏Å‡∏≥‡∏•‡∏±‡∏á scale vectors (embeddings)...\")\n",
    "scaler = StandardScaler(with_mean=False)\n",
    "X_train_vectors_sparse = csr_matrix(X_train_vectors)\n",
    "X_test_vectors_sparse  = csr_matrix(X_test_vectors)\n",
    "X_train_vectors_scaled = scaler.fit_transform(X_train_vectors_sparse)\n",
    "X_test_vectors_scaled  = scaler.transform(X_test_vectors_sparse)\n",
    "\n",
    "numerical_features = [c for c in feature_columns if c != 'cleaned_review']\n",
    "X_train_additional = X_train[numerical_features].values\n",
    "X_test_additional  = X_test[numerical_features].values\n",
    "\n",
    "features_scaler = StandardScaler()\n",
    "X_train_additional_scaled = features_scaler.fit_transform(X_train_additional)\n",
    "X_test_additional_scaled  = features_scaler.transform(X_test_additional)\n",
    "\n",
    "print(f\"TF-IDF features: {X_train_tfidf.shape[1]}\")\n",
    "print(f\"Count features: {X_train_count.shape[1]}\")\n",
    "print(f\"Word embedding features: {X_train_vectors.shape[1]}\")\n",
    "print(f\"Additional features: {X_train_additional.shape[1]}\")\n",
    "\n",
    "### **8. ‡∏£‡∏ß‡∏° Features ‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î**\n",
    "print(\"‡∏Å‡∏≥‡∏•‡∏±‡∏á‡∏£‡∏ß‡∏° features ‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î...\")\n",
    "X_train_combined = hstack([\n",
    "    X_train_tfidf,\n",
    "    X_train_count,\n",
    "    X_train_vectors_scaled,\n",
    "    csr_matrix(X_train_additional_scaled)\n",
    "], format='csr')\n",
    "\n",
    "X_test_combined = hstack([\n",
    "    X_test_tfidf,\n",
    "    X_test_count,\n",
    "    X_test_vectors_scaled,\n",
    "    csr_matrix(X_test_additional_scaled)\n",
    "], format='csr')\n",
    "\n",
    "print(f\"‡∏Ç‡∏ô‡∏≤‡∏î‡∏Ç‡∏≠‡∏á features ‡∏£‡∏ß‡∏° - train: {X_train_combined.shape}, test: {X_test_combined.shape}\")\n",
    "\n",
    "# ‡∏™‡∏≥‡∏£‡∏≠‡∏á‡πÅ‡∏ö‡∏ö dense ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡πÇ‡∏°‡πÄ‡∏î‡∏•‡∏ó‡∏µ‡πà‡πÑ‡∏°‡πà‡∏£‡∏≠‡∏á‡∏£‡∏±‡∏ö sparse\n",
    "print(\"‡πÅ‡∏õ‡∏•‡∏á dense ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏ö‡∏≤‡∏á‡πÇ‡∏°‡πÄ‡∏î‡∏•...\")\n",
    "X_train_combined_dense = X_train_combined.toarray()\n",
    "X_test_combined_dense  = X_test_combined.toarray()\n",
    "\n",
    "# ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö Naive Bayes ‡∏ï‡πâ‡∏≠‡∏á‡πÑ‡∏°‡πà‡πÄ‡∏õ‡πá‡∏ô‡∏•‡∏ö\n",
    "X_train_nb = np.abs(X_train_combined_dense)\n",
    "X_test_nb  = np.abs(X_test_combined_dense)\n",
    "\n",
    "### **9. ‡∏Å‡∏≥‡∏´‡∏ô‡∏î Models**\n",
    "models = {\n",
    "    'Logistic Regression': LogisticRegression(random_state=42, max_iter=1000, C=0.2,\n",
    "                                              class_weight='balanced', solver='saga',\n",
    "                                              multi_class='multinomial'),\n",
    "    'Random Forest': RandomForestClassifier(random_state=42, n_jobs=-1,\n",
    "                                            n_estimators=200, max_depth=15,\n",
    "                                            min_samples_split=5, min_samples_leaf=2,\n",
    "                                            class_weight='balanced'),\n",
    "    'Linear SVM': LinearSVC(random_state=42, max_iter=2000, C=0.5,\n",
    "                            class_weight='balanced', dual=False),\n",
    "    'Multinomial Naive Bayes': MultinomialNB(alpha=1.0, fit_prior=True)\n",
    "}\n",
    "\n",
    "### **10. ‡∏ù‡∏∂‡∏Å‡∏ù‡∏ô‡πÅ‡∏•‡∏∞‡∏õ‡∏£‡∏∞‡πÄ‡∏°‡∏¥‡∏ô‡∏ú‡∏• Models**\n",
    "results = {}\n",
    "best_models = {}\n",
    "training_times = {}\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"‡∏Å‡∏≥‡∏•‡∏±‡∏á‡∏ù‡∏∂‡∏Å‡∏ù‡∏ô‡πÅ‡∏•‡∏∞‡∏õ‡∏£‡∏∞‡πÄ‡∏°‡∏¥‡∏ô‡∏ú‡∏• MODELS\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "for model_name, model in models.items():\n",
    "    print(f\"\\nüîß ‡∏Å‡∏≥‡∏•‡∏±‡∏á‡∏ù‡∏∂‡∏Å‡∏ù‡∏ô {model_name}...\")\n",
    "    start_time = time.time()\n",
    "    \n",
    "    # ‡πÄ‡∏•‡∏∑‡∏≠‡∏Å‡∏ä‡∏∏‡∏î‡∏ü‡∏µ‡πÄ‡∏à‡∏≠‡∏£‡πå‡∏ó‡∏µ‡πà‡πÄ‡∏´‡∏°‡∏≤‡∏∞‡∏™‡∏°\n",
    "    if model_name == 'Multinomial Naive Bayes':\n",
    "        Xtr, Xte = X_train_nb, X_test_nb\n",
    "    elif model_name == 'Random Forest':\n",
    "        # Tree-based ‡πÑ‡∏°‡πà‡∏£‡∏±‡∏ö sparse -> ‡πÉ‡∏ä‡πâ dense\n",
    "        Xtr, Xte = X_train_combined_dense, X_test_combined_dense\n",
    "    else:\n",
    "        Xtr, Xte = X_train_combined, X_test_combined\n",
    "    \n",
    "    model.fit(Xtr, y_train)\n",
    "    best_models[model_name] = model\n",
    "    training_time = time.time() - start_time\n",
    "    training_times[model_name] = training_time\n",
    "    \n",
    "    y_pred = model.predict(Xte)\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "\n",
    "    # cross_val_score: ‡πÉ‡∏ä‡πâ‡∏ü‡∏µ‡πÄ‡∏à‡∏≠‡∏£‡πå‡πÄ‡∏î‡∏µ‡∏¢‡∏ß‡∏Å‡∏±‡∏ö‡∏ï‡∏≠‡∏ô train\n",
    "    try:\n",
    "        cv_scores = cross_val_score(model, Xtr, y_train, cv=5, scoring='accuracy')\n",
    "    except Exception:\n",
    "        # ‡πÄ‡∏ú‡∏∑‡πà‡∏≠‡∏ö‡∏≤‡∏á‡πÇ‡∏°‡πÄ‡∏î‡∏•‡πÑ‡∏°‡πà‡πÄ‡∏´‡∏°‡∏≤‡∏∞‡∏Å‡∏±‡∏ö CV ‡πÉ‡∏ô‡∏™‡∏†‡∏≤‡∏û‡∏ô‡∏µ‡πâ\n",
    "        cv_scores = np.array([accuracy])\n",
    "    \n",
    "    results[model_name] = {\n",
    "        'accuracy': accuracy,\n",
    "        'cv_mean': float(np.mean(cv_scores)),\n",
    "        'cv_std': float(np.std(cv_scores)),\n",
    "        'predictions': y_pred,\n",
    "        'training_time': training_time\n",
    "    }\n",
    "    \n",
    "    print(f\"‚úÖ {model_name} ‡πÄ‡∏™‡∏£‡πá‡∏à‡∏™‡∏¥‡πâ‡∏ô!\")\n",
    "    print(f\"   ‡∏Ñ‡∏ß‡∏≤‡∏°‡πÅ‡∏°‡πà‡∏ô‡∏¢‡∏≥‡∏Å‡∏≤‡∏£‡∏ó‡∏î‡∏™‡∏≠‡∏ö: {accuracy:.4f}\")\n",
    "    print(f\"   ‡∏Ñ‡∏ß‡∏≤‡∏°‡πÅ‡∏°‡πà‡∏ô‡∏¢‡∏≥ CV: {np.mean(cv_scores):.4f} (¬±{np.std(cv_scores):.4f})\")\n",
    "    print(f\"   ‡πÄ‡∏ß‡∏•‡∏≤‡∏ù‡∏∂‡∏Å‡∏ù‡∏ô: {training_time:.2f} ‡∏ß‡∏¥‡∏ô‡∏≤‡∏ó‡∏µ\")\n",
    "\n",
    "### **11. ‡πÄ‡∏õ‡∏£‡∏µ‡∏¢‡∏ö‡πÄ‡∏ó‡∏µ‡∏¢‡∏ö‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå‡πÅ‡∏•‡∏∞‡∏Å‡∏≤‡∏£‡πÅ‡∏™‡∏î‡∏á‡∏ú‡∏•**\n",
    "results_df = pd.DataFrame({\n",
    "    'Model': list(results.keys()),\n",
    "    'Test Accuracy': [results[m]['accuracy'] for m in results.keys()],\n",
    "    'CV Mean': [results[m]['cv_mean'] for m in results.keys()],\n",
    "    'CV Std': [results[m]['cv_std'] for m in results.keys()],\n",
    "    'Training Time (s)': [results[m]['training_time'] for m in results.keys()]\n",
    "})\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"‡∏ú‡∏•‡∏Å‡∏≤‡∏£‡πÄ‡∏õ‡∏£‡∏µ‡∏¢‡∏ö‡πÄ‡∏ó‡∏µ‡∏¢‡∏ö MODELS\")\n",
    "print(\"=\"*60)\n",
    "print(results_df.round(4))\n",
    "\n",
    "best_model_name = results_df.loc[results_df['Test Accuracy'].idxmax(), 'Model']\n",
    "print(f\"\\nüèÜ ‡πÇ‡∏°‡πÄ‡∏î‡∏•‡∏ó‡∏µ‡πà‡∏°‡∏µ‡∏õ‡∏£‡∏∞‡∏™‡∏¥‡∏ó‡∏ò‡∏¥‡∏†‡∏≤‡∏û‡∏î‡∏µ‡∏ó‡∏µ‡πà‡∏™‡∏∏‡∏î: {best_model_name}\")\n",
    "print(f\"   ‡∏Ñ‡∏ß‡∏≤‡∏°‡πÅ‡∏°‡πà‡∏ô‡∏¢‡∏≥‡∏Å‡∏≤‡∏£‡∏ó‡∏î‡∏™‡∏≠‡∏ö: {results[best_model_name]['accuracy']:.4f}\")\n",
    "\n",
    "### **12. ‡∏Å‡∏≤‡∏£‡πÅ‡∏™‡∏î‡∏á‡∏ú‡∏•‡πÅ‡∏ö‡∏ö‡∏Å‡∏£‡∏≤‡∏ü**\n",
    "plt.rcParams['font.family'] = ['DejaVu Sans']\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
    "\n",
    "axes[0, 0].bar(results_df['Model'], results_df['Test Accuracy'])\n",
    "axes[0, 0].set_title('‡∏Å‡∏≤‡∏£‡πÄ‡∏õ‡∏£‡∏µ‡∏¢‡∏ö‡πÄ‡∏ó‡∏µ‡∏¢‡∏ö‡∏Ñ‡∏ß‡∏≤‡∏°‡πÅ‡∏°‡πà‡∏ô‡∏¢‡∏≥‡∏Å‡∏≤‡∏£‡∏ó‡∏î‡∏™‡∏≠‡∏ö', fontsize=14, fontweight='bold')\n",
    "axes[0, 0].set_ylabel('‡∏Ñ‡∏ß‡∏≤‡∏°‡πÅ‡∏°‡πà‡∏ô‡∏¢‡∏≥'); axes[0, 0].tick_params(axis='x', rotation=45); axes[0, 0].grid(True, alpha=0.3)\n",
    "for i, v in enumerate(results_df['Test Accuracy']):\n",
    "    axes[0, 0].text(i, v + 0.005, f'{v:.3f}', ha='center', fontweight='bold')\n",
    "\n",
    "axes[0, 1].errorbar(range(len(results_df)), results_df['CV Mean'],\n",
    "                    yerr=results_df['CV Std'], fmt='o-', capsize=5, linewidth=2)\n",
    "axes[0, 1].set_title('‡∏Ñ‡∏ß‡∏≤‡∏°‡πÅ‡∏°‡πà‡∏ô‡∏¢‡∏≥ Cross-Validation (‡πÄ‡∏â‡∏•‡∏µ‡πà‡∏¢ ¬± ‡∏™‡πà‡∏ß‡∏ô‡πÄ‡∏ö‡∏µ‡πà‡∏¢‡∏á‡πÄ‡∏ö‡∏ô‡∏°‡∏≤‡∏ï‡∏£‡∏ê‡∏≤‡∏ô)', fontsize=14, fontweight='bold')\n",
    "axes[0, 1].set_ylabel('‡∏Ñ‡∏ß‡∏≤‡∏°‡πÅ‡∏°‡πà‡∏ô‡∏¢‡∏≥ CV'); axes[0, 1].set_xticks(range(len(results_df)))\n",
    "axes[0, 1].set_xticklabels(results_df['Model'], rotation=45); axes[0, 1].grid(True, alpha=0.3)\n",
    "\n",
    "bars = axes[1, 0].bar(results_df['Model'], results_df['Training Time (s)'])\n",
    "axes[1, 0].set_title('‡∏Å‡∏≤‡∏£‡πÄ‡∏õ‡∏£‡∏µ‡∏¢‡∏ö‡πÄ‡∏ó‡∏µ‡∏¢‡∏ö‡πÄ‡∏ß‡∏•‡∏≤‡∏ù‡∏∂‡∏Å‡∏ù‡∏ô', fontsize=14, fontweight='bold')\n",
    "axes[1, 0].set_ylabel('‡πÄ‡∏ß‡∏•‡∏≤ (‡∏ß‡∏¥‡∏ô‡∏≤‡∏ó‡∏µ)'); axes[1, 0].tick_params(axis='x', rotation=45); axes[1, 0].grid(True, alpha=0.3)\n",
    "for bar, time_val in zip(bars, results_df['Training Time (s)']):\n",
    "    axes[1, 0].text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.1, f'{time_val:.1f}s', ha='center', fontweight='bold')\n",
    "\n",
    "# Radar\n",
    "categories = ['‡∏Ñ‡∏ß‡∏≤‡∏°‡πÅ‡∏°‡πà‡∏ô‡∏¢‡∏≥‡∏ó‡∏î‡∏™‡∏≠‡∏ö', '‡∏Ñ‡∏ß‡∏≤‡∏°‡πÅ‡∏°‡πà‡∏ô‡∏¢‡∏≥ CV', '‡∏Ñ‡∏ß‡∏≤‡∏°‡πÄ‡∏£‡πá‡∏ß (‡∏Å‡∏•‡∏±‡∏ö‡∏´‡∏±‡∏ß)']\n",
    "fig2, ax = plt.subplots(figsize=(8, 8), subplot_kw=dict(projection='polar'))\n",
    "normalized_accuracy = results_df['Test Accuracy'] / max(1e-9, results_df['Test Accuracy'].max())\n",
    "normalized_cv = results_df['CV Mean'] / max(1e-9, results_df['CV Mean'].max())\n",
    "normalized_speed = (1 / (results_df['Training Time (s)'] + 0.1))\n",
    "normalized_speed = normalized_speed / max(1e-9, normalized_speed.max())\n",
    "angles = np.linspace(0, 2*np.pi, len(categories), endpoint=False).tolist(); angles += angles[:1]\n",
    "base_colors = ['C0','C1','C2','C3','C4','C5','C6','C7','C8','C9']\n",
    "for i, model in enumerate(results_df['Model']):\n",
    "    values = [normalized_accuracy.iloc[i], normalized_cv.iloc[i], normalized_speed.iloc[i]]; values += values[:1]\n",
    "    ax.plot(angles, values, 'o-', linewidth=2, label=model, color=base_colors[i % len(base_colors)])\n",
    "    ax.fill(angles, values, alpha=0.25, color=base_colors[i % len(base_colors)])\n",
    "ax.set_xticks(angles[:-1]); ax.set_xticklabels(categories); ax.set_ylim(0, 1)\n",
    "ax.set_title('‡∏Å‡∏≤‡∏£‡πÄ‡∏õ‡∏£‡∏µ‡∏¢‡∏ö‡πÄ‡∏ó‡∏µ‡∏¢‡∏ö‡∏õ‡∏£‡∏∞‡∏™‡∏¥‡∏ó‡∏ò‡∏¥‡∏†‡∏≤‡∏û‡πÇ‡∏°‡πÄ‡∏î‡∏• (‡∏õ‡∏£‡∏±‡∏ö‡∏°‡∏≤‡∏ï‡∏£‡∏ê‡∏≤‡∏ô)', fontweight='bold', size=14)\n",
    "ax.legend(loc='upper right', bbox_to_anchor=(1.2, 1.0)); ax.grid(True)\n",
    "axes[1, 1].remove()\n",
    "plt.tight_layout(); plt.show()\n",
    "\n",
    "### **13. ‡∏£‡∏≤‡∏¢‡∏á‡∏≤‡∏ô‡∏Å‡∏≤‡∏£‡∏à‡∏≥‡πÅ‡∏ô‡∏Å‡∏õ‡∏£‡∏∞‡πÄ‡∏†‡∏ó‡πÇ‡∏î‡∏¢‡∏•‡∏∞‡πÄ‡∏≠‡∏µ‡∏¢‡∏î**\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"‡∏£‡∏≤‡∏¢‡∏á‡∏≤‡∏ô‡∏Å‡∏≤‡∏£‡∏à‡∏≥‡πÅ‡∏ô‡∏Å‡∏õ‡∏£‡∏∞‡πÄ‡∏†‡∏ó‡πÇ‡∏î‡∏¢‡∏•‡∏∞‡πÄ‡∏≠‡∏µ‡∏¢‡∏î\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# ‡∏ó‡∏≥‡πÉ‡∏´‡πâ‡∏õ‡∏•‡∏≠‡∏î‡∏†‡∏±‡∏¢‡∏´‡∏≤‡∏Å‡∏ö‡∏≤‡∏á‡∏Ñ‡∏•‡∏≤‡∏™‡πÑ‡∏°‡πà‡∏°‡∏µ‡πÉ‡∏ô y_test\n",
    "unique_labels = sorted(df['rating'].unique().tolist())\n",
    "target_names = [f'‡∏Ñ‡∏∞‡πÅ‡∏ô‡∏ô {i}' for i in unique_labels]\n",
    "\n",
    "for model_name in results.keys():\n",
    "    print(f\"\\nüìä {model_name}\")\n",
    "    print(\"-\" * 40)\n",
    "    model = models[model_name]\n",
    "    params_show = ['C','class_weight','solver','multi_class','n_estimators',\n",
    "                   'max_depth','min_samples_split','min_samples_leaf','dual',\n",
    "                   'alpha','fit_prior']\n",
    "    param_desc = {\n",
    "        'C':'‡∏Ñ‡πà‡∏≤ regularization parameter','class_weight':'‡∏Å‡∏≤‡∏£‡∏õ‡∏£‡∏±‡∏ö‡∏ô‡πâ‡∏≥‡∏´‡∏ô‡∏±‡∏Å‡∏Ñ‡∏•‡∏≤‡∏™',\n",
    "        'solver':'‡∏≠‡∏±‡∏•‡∏Å‡∏≠‡∏£‡∏¥‡∏ó‡∏∂‡∏°‡∏Å‡∏≤‡∏£‡∏´‡∏≤‡∏Ñ‡πà‡∏≤‡∏ó‡∏µ‡πà‡πÄ‡∏´‡∏°‡∏≤‡∏∞‡∏™‡∏°','multi_class':'‡∏ß‡∏¥‡∏ò‡∏µ‡∏Å‡∏≤‡∏£‡∏à‡∏±‡∏î‡∏Å‡∏≤‡∏£‡∏´‡∏•‡∏≤‡∏¢‡∏Ñ‡∏•‡∏≤‡∏™',\n",
    "        'n_estimators':'‡∏à‡∏≥‡∏ô‡∏ß‡∏ô decision trees','max_depth':'‡∏Ñ‡∏ß‡∏≤‡∏°‡∏•‡∏∂‡∏Å‡∏™‡∏π‡∏á‡∏™‡∏∏‡∏î‡∏Ç‡∏≠‡∏á‡∏ï‡πâ‡∏ô‡πÑ‡∏°‡πâ',\n",
    "        'min_samples_split':'‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏Ç‡∏±‡πâ‡∏ô‡∏ï‡πà‡∏≥‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡πÅ‡∏¢‡∏Å node',\n",
    "        'min_samples_leaf':'‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏Ç‡∏±‡πâ‡∏ô‡∏ï‡πà‡∏≥‡πÉ‡∏ô leaf node',\n",
    "        'dual':'‡∏Å‡∏≤‡∏£‡πÉ‡∏ä‡πâ dual formulation','alpha':'‡∏Ñ‡πà‡∏≤ smoothing parameter',\n",
    "        'fit_prior':'‡∏Å‡∏≤‡∏£‡πÉ‡∏ä‡πâ prior probability'\n",
    "    }\n",
    "    for p, v in model.get_params().items():\n",
    "        if p in params_show:\n",
    "            print(f\"  {p}: {v} # {param_desc.get(p,'')}\")\n",
    "    print(f\"\\n‡∏£‡∏≤‡∏¢‡∏á‡∏≤‡∏ô‡∏Å‡∏≤‡∏£‡∏à‡∏≥‡πÅ‡∏ô‡∏Å‡∏õ‡∏£‡∏∞‡πÄ‡∏†‡∏ó:\")\n",
    "    # ‡πÉ‡∏ä‡πâ labels ‡πÅ‡∏•‡∏∞ target_names ‡πÉ‡∏´‡πâ‡∏™‡∏≠‡∏î‡∏Ñ‡∏•‡πâ‡∏≠‡∏á‡∏Å‡∏±‡∏ô\n",
    "    print(classification_report(y_test, results[model_name]['predictions'],\n",
    "                                labels=unique_labels, target_names=target_names, zero_division=0))\n",
    "\n",
    "### **14. Confusion Matrices**\n",
    "rows = int(np.ceil(len(results)/2))\n",
    "cols = 2 if len(results) > 1 else 1\n",
    "fig, axes = plt.subplots(rows, cols, figsize=(15, 6*rows))\n",
    "axes = np.array(axes).reshape(-1)\n",
    "for i, (model_name, result) in enumerate(results.items()):\n",
    "    cm = confusion_matrix(y_test, result['predictions'], labels=unique_labels)\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
    "                xticklabels=[str(l) for l in unique_labels],\n",
    "                yticklabels=[str(l) for l in unique_labels],\n",
    "                ax=axes[i])\n",
    "    axes[i].set_xlabel(\"‡∏Ñ‡∏∞‡πÅ‡∏ô‡∏ô‡∏ó‡∏µ‡πà‡∏ó‡∏≥‡∏ô‡∏≤‡∏¢\"); axes[i].set_ylabel(\"‡∏Ñ‡∏∞‡πÅ‡∏ô‡∏ô‡∏à‡∏£‡∏¥‡∏á\")\n",
    "    axes[i].set_title(f\"{model_name}\\n‡∏Ñ‡∏ß‡∏≤‡∏°‡πÅ‡∏°‡πà‡∏ô‡∏¢‡∏≥: {result['accuracy']:.3f}\")\n",
    "plt.tight_layout(); plt.savefig(\"confusion_matrices_comparison.png\", dpi=300, bbox_inches='tight'); plt.show()\n",
    "\n",
    "### **15. ‡∏ü‡∏±‡∏á‡∏Å‡πå‡∏ä‡∏±‡∏ô‡∏ó‡∏≥‡∏ô‡∏≤‡∏¢‡∏ú‡∏•‡∏ó‡∏µ‡πà‡∏õ‡∏£‡∏±‡∏ö‡∏õ‡∏£‡∏∏‡∏á**\n",
    "def predict_review_enhanced(text, model_name='best'):\n",
    "    if model_name == 'best':\n",
    "        model_name = best_model_name\n",
    "    model = best_models[model_name]\n",
    "    cleaned_text = clean_text(text)\n",
    "    features = extract_features(cleaned_text)\n",
    "\n",
    "    review_vector = enhanced_sentence_vectorizer(cleaned_text).reshape(1, -1)\n",
    "    review_vector_sparse = csr_matrix(review_vector)\n",
    "    review_vector_scaled = scaler.transform(review_vector_sparse)\n",
    "\n",
    "    review_tfidf = tfidf_vectorizer.transform([cleaned_text])\n",
    "    review_count = count_vectorizer.transform([cleaned_text])\n",
    "\n",
    "    additional_features = np.array([[\n",
    "        features['exclamation_count'], features['question_count'],\n",
    "        features['sentence_count'], features['word_count'],\n",
    "        features['avg_word_length'], features['repeated_words_ratio'],\n",
    "        features['negation_count'], features['punctuation_ratio'],\n",
    "        features['text_length'], features['words_per_sentence'],\n",
    "        features['english_ratio']\n",
    "    ]], dtype=np.float32)\n",
    "    additional_features_scaled = features_scaler.transform(additional_features)\n",
    "\n",
    "    if model_name in ['Multinomial Naive Bayes','Random Forest']:\n",
    "        # ‡πÉ‡∏ä‡πâ‡πÄ‡∏ß‡∏≠‡∏£‡πå‡∏ä‡∏±‡∏ô dense ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡πÇ‡∏°‡πÄ‡∏î‡∏•‡∏ó‡∏µ‡πà‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£ non-negative/dense\n",
    "        review_combined = hstack([\n",
    "            review_tfidf, review_count, review_vector_scaled, csr_matrix(additional_features_scaled)\n",
    "        ], format='csr').toarray()\n",
    "        if model_name == 'Multinomial Naive Bayes':\n",
    "            review_combined = np.abs(review_combined)\n",
    "    else:\n",
    "        review_combined = hstack([\n",
    "            review_tfidf, review_count, review_vector_scaled, csr_matrix(additional_features_scaled)\n",
    "        ], format='csr')\n",
    "\n",
    "    pred = model.predict(review_combined)[0]\n",
    "    if hasattr(model, 'predict_proba'):\n",
    "        probs = model.predict_proba(review_combined)[0]\n",
    "    else:\n",
    "        # LinearSVC\n",
    "        scores = model.decision_function(review_combined)[0]\n",
    "        # ‡∏õ‡∏£‡∏±‡∏ö softmax ‡πÅ‡∏ö‡∏ö‡∏á‡πà‡∏≤‡∏¢\n",
    "        exps = np.exp(scores - np.max(scores))\n",
    "        probs = exps / np.sum(exps)\n",
    "    return int(pred), probs\n",
    "\n",
    "### **16. ‡∏ó‡∏î‡∏™‡∏≠‡∏ö‡∏Å‡∏±‡∏ö‡∏£‡∏µ‡∏ß‡∏¥‡∏ß‡∏à‡∏£‡∏¥‡∏á**\n",
    "def print_prediction_results_enhanced(text, actual_rating, model_name='best'):\n",
    "    predicted_class, confidences = predict_review_enhanced(text, model_name)\n",
    "    is_correct = int(predicted_class) == int(actual_rating)\n",
    "    print(f\"\\n‡∏£‡∏µ‡∏ß‡∏¥‡∏ß: {text[:100]}...\")\n",
    "    print(f\"‡πÇ‡∏°‡πÄ‡∏î‡∏•: {model_name}\")\n",
    "    print(f\"‡∏Ñ‡∏∞‡πÅ‡∏ô‡∏ô‡∏à‡∏£‡∏¥‡∏á: {actual_rating}/5\")\n",
    "    print(f\"‡∏Ñ‡∏∞‡πÅ‡∏ô‡∏ô‡∏ó‡∏µ‡πà‡∏ó‡∏≥‡∏ô‡∏≤‡∏¢: {predicted_class}/5\")\n",
    "    print(f\"‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå: {'‚úÖ ‡∏ñ‡∏π‡∏Å‡∏ï‡πâ‡∏≠‡∏á' if is_correct else '‚ùå ‡∏ú‡∏¥‡∏î‡∏û‡∏•‡∏≤‡∏î'}\")\n",
    "    print(\"‡∏Ñ‡∏ß‡∏≤‡∏°‡∏°‡∏±‡πà‡∏ô‡πÉ‡∏à:\")\n",
    "    # ‡∏ó‡∏≥ mapping ‡∏ï‡∏≤‡∏°‡∏•‡∏≥‡∏î‡∏±‡∏ö‡∏Ñ‡∏•‡∏≤‡∏™‡∏ó‡∏µ‡πà‡πÇ‡∏°‡πÄ‡∏î‡∏•‡πÉ‡∏ä‡πâ (‡∏™‡∏°‡∏°‡∏ï‡∏¥‡πÄ‡∏£‡∏µ‡∏¢‡∏á 1..5)\n",
    "    for rating_idx, confidence in enumerate(confidences, 1):\n",
    "        print(f\"  ‚≠ê {rating_idx}: {confidence * 100:.1f}%\")\n",
    "    return predicted_class, int(actual_rating)\n",
    "\n",
    "test_reviews = [\n",
    "    [\"‡πÑ‡∏°‡πà‡πÅ‡∏ô‡∏∞‡∏ô‡∏≥‡πÄ‡∏•‡∏¢‡∏Ñ‡πà‡∏∞ ‡∏´‡∏≠‡∏ô‡∏µ‡πâ ‡∏´‡∏•‡∏≠‡∏Å‡πÄ‡∏≠‡∏≤‡πÄ‡∏á‡∏¥‡∏ô‡∏ä‡∏±‡∏î‡πÜ ‡πÉ‡∏ô‡∏£‡∏π‡∏õ‡∏™‡∏ß‡∏¢‡∏°‡∏≤‡∏Å ‡πÅ‡∏ï‡πà‡∏û‡∏≠‡πÄ‡∏Ç‡πâ‡∏≤‡πÑ‡∏õ‡∏≠‡∏¢‡∏π‡πà‡∏à‡∏£‡∏¥‡∏á‡∏™‡∏†‡∏≤‡∏û‡∏´‡πâ‡∏≠‡∏á‡∏ó‡∏£‡∏∏‡∏î‡πÇ‡∏ó‡∏£‡∏°‡∏°‡∏≤‡∏Å ‡∏ï‡∏π‡πâ‡πÄ‡∏™‡∏∑‡πâ‡∏≠‡∏ú‡πâ‡∏≤‡∏û‡∏±‡∏á‡∏ï‡∏±‡πâ‡∏á‡πÅ‡∏ï‡πà‡∏ß‡∏±‡∏ô‡πÅ‡∏£‡∏Å‡∏ó‡∏µ‡πà‡∏¢‡πâ‡∏≤‡∏¢‡πÄ‡∏Ç‡πâ‡∏≤ ‡πÄ‡∏ï‡∏µ‡∏¢‡∏á‡∏Å‡πá‡πÄ‡∏Å‡πà‡∏≤‡∏°‡∏≤‡∏Å‡∏ô‡∏≠‡∏ô‡πÅ‡∏•‡πâ‡∏ß‡∏õ‡∏ß‡∏î‡∏´‡∏•‡∏±‡∏á ‡∏ù‡∏±‡∏Å‡∏ö‡∏±‡∏ß‡∏ô‡πâ‡∏≥‡∏Å‡πá‡πÑ‡∏´‡∏•‡πÅ‡∏Ñ‡πà‡∏ã‡∏¥‡∏Å‡πÜ ‡πÑ‡∏°‡πà‡πÄ‡∏Ñ‡∏¢‡∏°‡∏≤‡∏ã‡πà‡∏≠‡∏°‡πÉ‡∏´‡πâ‡∏™‡∏±‡∏Å‡∏ó‡∏µ ‡∏Ç‡∏≠‡∏¢‡πâ‡∏≤‡∏¢‡∏≠‡∏≠‡∏Å‡∏Å‡πá‡πÑ‡∏°‡πà‡∏Ñ‡∏∑‡∏ô‡πÄ‡∏á‡∏¥‡∏ô‡∏°‡∏±‡∏î‡∏à‡∏≥ ‡πÄ‡∏™‡∏µ‡∏¢‡∏Ñ‡∏ß‡∏≤‡∏°‡∏£‡∏π‡πâ‡∏™‡∏∂‡∏Å‡∏°‡∏≤‡∏Å‡∏Ñ‡πà‡∏∞\", 1],\n",
    "    [\"‡∏´‡∏≠‡∏û‡∏±‡∏Å‡∏£‡∏≤‡∏Ñ‡∏≤‡∏Å‡πá‡πÇ‡∏≠‡πÄ‡∏Ñ‡∏ô‡∏∞ ‡πÑ‡∏°‡πà‡πÅ‡∏û‡∏á‡∏°‡∏≤‡∏Å ‡πÅ‡∏ï‡πà‡∏°‡∏µ‡∏Ç‡πâ‡∏≠‡πÄ‡∏™‡∏µ‡∏¢‡πÄ‡∏¢‡∏≠‡∏∞‡πÑ‡∏õ‡∏´‡∏ô‡πà‡∏≠‡∏¢ ‡∏´‡πâ‡∏≠‡∏á‡πÄ‡∏•‡πá‡∏Å‡πÄ‡∏Å‡∏¥‡∏ô‡πÑ‡∏õ ‡πÅ‡∏≠‡∏£‡πå‡πÄ‡∏™‡∏µ‡∏¢‡∏á‡∏î‡∏±‡∏á‡∏£‡∏ö‡∏Å‡∏ß‡∏ô‡πÄ‡∏ß‡∏•‡∏≤‡∏ô‡∏≠‡∏ô ‡∏õ‡∏£‡∏∞‡∏ï‡∏π‡∏´‡πâ‡∏≠‡∏á‡∏ô‡πâ‡∏≥‡∏õ‡∏¥‡∏î‡πÑ‡∏°‡πà‡∏™‡∏ô‡∏¥‡∏ó ‡πÅ‡∏•‡πâ‡∏ß‡∏Å‡πá‡∏°‡∏µ‡∏°‡∏î‡πÄ‡∏¢‡∏≠‡∏∞‡∏°‡∏≤‡∏Å ‡∏Ç‡πâ‡∏≠‡∏î‡∏µ‡∏Ñ‡∏∑‡∏≠‡πÉ‡∏Å‡∏•‡πâ‡∏ï‡∏•‡∏≤‡∏î ‡πÄ‡∏î‡∏¥‡∏ô‡πÑ‡∏õ‡∏ã‡∏∑‡πâ‡∏≠‡∏Ç‡∏≠‡∏á‡∏Å‡∏¥‡∏ô‡πÑ‡∏î‡πâ‡∏™‡∏∞‡∏î‡∏ß‡∏Å ‡πÅ‡∏ï‡πà‡∏†‡∏≤‡∏û‡∏£‡∏ß‡∏°‡∏¢‡∏±‡∏á‡πÑ‡∏°‡πà‡∏Ñ‡∏∏‡πâ‡∏°‡∏Ñ‡πà‡∏≤‡πÄ‡∏ó‡πà‡∏≤‡πÑ‡∏´‡∏£‡πà ‡∏ñ‡πâ‡∏≤‡∏°‡∏µ‡∏ó‡∏≤‡∏á‡πÄ‡∏•‡∏∑‡∏≠‡∏Å‡∏≠‡∏∑‡πà‡∏ô‡∏Å‡πá‡∏ô‡πà‡∏≤‡∏à‡∏∞‡∏î‡∏µ‡∏Å‡∏ß‡πà‡∏≤‡∏ô‡∏∞\", 2],\n",
    "    [\"‡∏ä‡∏≠‡∏ö‡∏´‡∏≠‡∏ô‡∏µ‡πâ‡∏°‡∏≤‡∏Å‡∏Ñ‡πà‡∏∞ ‡∏´‡πâ‡∏≠‡∏á‡∏Å‡∏ß‡πâ‡∏≤‡∏á‡∏™‡∏∞‡∏≠‡∏≤‡∏î ‡πÄ‡∏ü‡∏≠‡∏£‡πå‡∏ô‡∏¥‡πÄ‡∏à‡∏≠‡∏£‡πå‡∏Ñ‡∏£‡∏ö‡∏Ñ‡∏£‡∏±‡∏ô ‡πÅ‡∏≠‡∏£‡πå‡πÄ‡∏¢‡πá‡∏ô‡∏â‡πà‡∏≥ ‡∏°‡∏µ‡πÇ‡∏ï‡πä‡∏∞‡πÄ‡∏Ñ‡∏£‡∏∑‡πà‡∏≠‡∏á‡πÅ‡∏õ‡πâ‡∏á‡∏î‡πâ‡∏ß‡∏¢ ‡∏™‡∏∞‡∏î‡∏ß‡∏Å‡∏°‡∏≤‡∏Å ‡∏≠‡∏¥‡∏ô‡πÄ‡∏ó‡∏≠‡∏£‡πå‡πÄ‡∏ô‡πá‡∏ï‡πÄ‡∏£‡πá‡∏ß ‡πÄ‡∏•‡πà‡∏ô‡πÄ‡∏Å‡∏°‡∏™‡∏ö‡∏≤‡∏¢ ‡πÄ‡∏à‡πâ‡∏≤‡∏Ç‡∏≠‡∏á‡∏´‡∏≠‡πÉ‡∏à‡∏î‡∏µ ‡∏°‡∏µ‡∏≠‡∏∞‡πÑ‡∏£‡πÅ‡∏à‡πâ‡∏á‡∏õ‡∏∏‡πä‡∏ö‡∏°‡∏≤‡∏î‡∏π‡∏õ‡∏±‡πä‡∏ö ‡∏Ç‡πâ‡∏≠‡πÄ‡∏™‡∏µ‡∏¢‡πÄ‡∏•‡πá‡∏Å‡πÜ‡∏Ñ‡∏∑‡∏≠‡∏Ñ‡πà‡∏≤‡πÑ‡∏ü‡∏Ñ‡πà‡∏≠‡∏ô‡∏Ç‡πâ‡∏≤‡∏á‡πÅ‡∏û‡∏á ‡πÅ‡∏•‡πâ‡∏ß‡∏Å‡πá‡∏ã‡∏±‡∏Å‡∏ú‡πâ‡∏≤‡∏ï‡πâ‡∏≠‡∏á‡∏•‡∏á‡πÑ‡∏õ‡∏ä‡∏±‡πâ‡∏ô‡∏•‡πà‡∏≤‡∏á ‡∏≠‡∏¢‡∏≤‡∏Å‡πÉ‡∏´‡πâ‡∏°‡∏µ‡πÄ‡∏Ñ‡∏£‡∏∑‡πà‡∏≠‡∏á‡∏ã‡∏±‡∏Å‡∏ú‡πâ‡∏≤‡∏ó‡∏∏‡∏Å‡∏ä‡∏±‡πâ‡∏ô ‡πÅ‡∏ï‡πà‡πÇ‡∏î‡∏¢‡∏£‡∏ß‡∏°‡∏û‡∏≠‡πÉ‡∏à‡∏°‡∏≤‡∏Å‡∏Ñ‡πà‡∏∞ ‡πÅ‡∏ô‡∏∞‡∏ô‡∏≥‡πÄ‡∏•‡∏¢\", 4],\n",
    "    [\"Terrible hotel experience! The room was dirty, smelly, and completely different from the photos. Staff was rude and unhelpful. AC didn't work, hot water was cold, and WiFi was extremely slow. The bed was uncomfortable and the bathroom was disgusting. Would never stay here again. Complete waste of money!\", 1],\n",
    "    [\"The hotel is okay for the price. Room was decent size but could be cleaner. Staff was friendly but service was slow. Location is good, close to attractions. Some facilities need maintenance. WiFi worked well. Overall, it's an average hotel - nothing special but acceptable for a short stay.\", 3],\n",
    "    [\"Amazing hotel! Absolutely loved our stay here. The room was spacious, clean, and beautifully decorated. Staff was incredibly friendly and helpful. The location is perfect - walking distance to everything. Breakfast was delicious with great variety. Pool and gym facilities were excellent. Highly recommend this place!\", 5],\n",
    "    [\"This hotel is really good! ‡∏´‡πâ‡∏≠‡∏á‡∏™‡∏∞‡∏≠‡∏≤‡∏î‡∏°‡∏≤‡∏Å ‡πÅ‡∏≠‡∏£‡πå‡πÄ‡∏¢‡πá‡∏ô wifi super fast ‡πÅ‡∏ï‡πà‡∏£‡∏≤‡∏Ñ‡∏≤‡πÅ‡∏û‡∏á‡πÑ‡∏õ‡∏´‡∏ô‡πà‡∏≠‡∏¢ but overall worth it ‡∏ô‡∏∞ staff friendly ‡∏°‡∏≤‡∏Å highly recommended! üëç\", 4],\n",
    "    [\"‡∏´‡∏≠‡∏ô‡∏µ‡πâ‡∏î‡∏µ‡∏ó‡∏µ‡πà‡∏™‡∏∏‡∏î‡πÉ‡∏ô‡∏¢‡πà‡∏≤‡∏ô‡∏ô‡∏µ‡πâ‡πÅ‡∏•‡πâ‡∏ß‡∏ß ‡∏≠‡∏¢‡∏π‡πà‡∏°‡∏≤ 3 ‡∏õ‡∏µ‡πÑ‡∏°‡πà‡πÄ‡∏Ñ‡∏¢‡∏°‡∏µ‡∏õ‡∏±‡∏ç‡∏´‡∏≤‡πÄ‡∏•‡∏¢ ‡∏´‡πâ‡∏≠‡∏á‡∏Å‡∏ß‡πâ‡∏≤‡∏á ‡∏™‡∏∞‡∏≠‡∏≤‡∏î ‡∏ï‡∏Å‡πÅ‡∏ï‡πà‡∏á‡∏™‡∏ß‡∏¢ ‡∏°‡∏µ‡πÄ‡∏ü‡∏≠‡∏£‡πå‡∏Ñ‡∏£‡∏ö ‡πÄ‡∏´‡∏°‡∏∑‡∏≠‡∏ô‡∏≠‡∏¢‡∏π‡πà‡∏Ñ‡∏≠‡∏ô‡πÇ‡∏î ‡πÄ‡∏ô‡πá‡∏ï‡πÑ‡∏ß‡∏°‡∏≤‡∏Å 100 Mbps ‡πÄ‡∏•‡πà‡∏ô‡πÄ‡∏Å‡∏°‡πÑ‡∏°‡πà‡∏°‡∏µ‡∏™‡∏∞‡∏î‡∏∏‡∏î! ‡∏£‡∏∞‡∏ö‡∏ö‡∏£‡∏±‡∏Å‡∏©‡∏≤‡∏Ñ‡∏ß‡∏≤‡∏°‡∏õ‡∏•‡∏≠‡∏î‡∏†‡∏±‡∏¢‡πÅ‡∏ô‡πà‡∏ô‡∏°‡∏≤‡∏Å ‡∏°‡∏µ‡∏Å‡∏•‡πâ‡∏≠‡∏á‡∏ß‡∏á‡∏à‡∏£‡∏õ‡∏¥‡∏î ‡∏Ñ‡∏µ‡∏¢‡πå‡∏Å‡∏≤‡∏£‡πå‡∏î‡∏ó‡∏∏‡∏Å‡∏ä‡∏±‡πâ‡∏ô ‡πÅ‡∏•‡∏∞‡∏°‡∏µ ‡∏£‡∏õ‡∏†. 24 ‡∏ä‡∏°. ‡∏ó‡∏µ‡πÄ‡∏î‡πá‡∏î‡∏™‡∏∏‡∏î‡∏Ñ‡∏∑‡∏≠‡∏°‡∏µ‡∏ü‡∏¥‡∏ï‡πÄ‡∏ô‡∏™‡πÅ‡∏•‡∏∞‡∏™‡∏£‡∏∞‡∏ß‡πà‡∏≤‡∏¢‡∏ô‡πâ‡∏≥‡πÉ‡∏´‡πâ‡πÉ‡∏ä‡πâ‡∏ü‡∏£‡∏µ ‡∏Ñ‡∏∏‡πâ‡∏°‡∏°‡∏≤‡∏Å‡∏Å‡∏Å‡∏Å ‡πÅ‡∏ô‡∏∞‡∏ô‡∏≥‡∏™‡∏∏‡∏î‡πÜ ‡∏ñ‡πâ‡∏≤‡πÑ‡∏î‡πâ‡∏´‡πâ‡∏≠‡∏á‡∏Å‡πá‡∏à‡∏≠‡∏á‡πÄ‡∏•‡∏¢‡∏≠‡∏¢‡πà‡∏≤‡∏£‡∏≠!\", 5]\n",
    "]\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"‡∏Å‡∏≤‡∏£‡∏ó‡∏î‡∏™‡∏≠‡∏ö‡∏Å‡∏±‡∏ö‡∏£‡∏µ‡∏ß‡∏¥‡∏ß‡∏à‡∏£‡∏¥‡∏á\")\n",
    "print(\"=\"*60)\n",
    "print(f\"üèÜ ‡∏ó‡∏î‡∏™‡∏≠‡∏ö‡∏Å‡∏±‡∏ö‡πÇ‡∏°‡πÄ‡∏î‡∏•‡∏ó‡∏µ‡πà‡∏î‡∏µ‡∏ó‡∏µ‡πà‡∏™‡∏∏‡∏î: {best_model_name}\")\n",
    "test_results = []\n",
    "for review, actual_rating in test_reviews:\n",
    "    predicted_rating, actual = print_prediction_results_enhanced(review, actual_rating, best_model_name)\n",
    "    test_results.append((predicted_rating, actual))\n",
    "\n",
    "correct_predictions = sum(1 for pred, actual in test_results if pred == actual)\n",
    "test_accuracy = correct_predictions / len(test_results)\n",
    "print(f\"\\nüìà ‡∏ú‡∏•‡∏™‡∏£‡∏∏‡∏õ‡∏Å‡∏≤‡∏£‡∏ó‡∏î‡∏™‡∏≠‡∏ö‡∏£‡∏µ‡∏ß‡∏¥‡∏ß (‡πÇ‡∏°‡πÄ‡∏î‡∏•‡∏ó‡∏µ‡πà‡∏î‡∏µ‡∏ó‡∏µ‡πà‡∏™‡∏∏‡∏î: {best_model_name}):\")\n",
    "print(f\"‡∏ó‡∏≥‡∏ô‡∏≤‡∏¢‡∏ñ‡∏π‡∏Å‡∏ï‡πâ‡∏≠‡∏á: {correct_predictions} ‡∏£‡∏µ‡∏ß‡∏¥‡∏ß\")\n",
    "print(f\"‡∏ó‡∏≥‡∏ô‡∏≤‡∏¢‡∏ú‡∏¥‡∏î‡∏û‡∏•‡∏≤‡∏î: {len(test_results) - correct_predictions} ‡∏£‡∏µ‡∏ß‡∏¥‡∏ß\")\n",
    "print(f\"‡∏Ñ‡∏ß‡∏≤‡∏°‡πÅ‡∏°‡πà‡∏ô‡∏¢‡∏≥‡∏£‡∏ß‡∏°: {test_accuracy:.2f} ({correct_predictions}/{len(test_results)})\")\n",
    "\n",
    "### **17. ‡πÄ‡∏õ‡∏£‡∏µ‡∏¢‡∏ö‡πÄ‡∏ó‡∏µ‡∏¢‡∏ö‡πÇ‡∏°‡πÄ‡∏î‡∏•‡∏Å‡∏±‡∏ö‡∏£‡∏µ‡∏ß‡∏¥‡∏ß‡∏ó‡∏î‡∏™‡∏≠‡∏ö**\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"‡πÄ‡∏õ‡∏£‡∏µ‡∏¢‡∏ö‡πÄ‡∏ó‡∏µ‡∏¢‡∏ö‡∏ó‡∏∏‡∏Å‡πÇ‡∏°‡πÄ‡∏î‡∏•‡∏Å‡∏±‡∏ö‡∏£‡∏µ‡∏ß‡∏¥‡∏ß‡∏ó‡∏î‡∏™‡∏≠‡∏ö\")\n",
    "print(\"=\"*60)\n",
    "model_test_results = {}\n",
    "for model_name in results.keys():\n",
    "    print(f\"\\nüîç ‡∏ó‡∏î‡∏™‡∏≠‡∏ö {model_name}:\")\n",
    "    model_results = []\n",
    "    for idx, (review, actual_rating) in enumerate(test_reviews[:5]):\n",
    "        predicted_rating, _ = predict_review_enhanced(review, model_name)\n",
    "        model_results.append(predicted_rating == actual_rating)\n",
    "        print(f\"  ‡∏£‡∏µ‡∏ß‡∏¥‡∏ß‡∏ó‡∏µ‡πà {idx+1}: {'‚úÖ' if model_results[-1] else '‚ùå'}\")\n",
    "    accuracy_small = sum(model_results) / len(model_results)\n",
    "    model_test_results[model_name] = accuracy_small\n",
    "    print(f\"  ‡∏Ñ‡∏ß‡∏≤‡∏°‡πÅ‡∏°‡πà‡∏ô‡∏¢‡∏≥‡πÉ‡∏ô‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á: {accuracy_small:.2f}\")\n",
    "\n",
    "### **18. ‡∏Å‡∏≤‡∏£‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå Feature Importance (‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö tree-based models)**\n",
    "if 'Random Forest' in best_models:\n",
    "    print(f\"\\nüå≥ ‡∏Å‡∏≤‡∏£‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç‡∏Ç‡∏≠‡∏á Features (Random Forest):\")\n",
    "    rf_model = best_models['Random Forest']\n",
    "    tfidf_features = [f\"tfidf_{i}\" for i in range(X_train_tfidf.shape[1])]\n",
    "    count_features = [f\"count_{i}\" for i in range(X_train_count.shape[1])]\n",
    "    embedding_features = [f\"embed_{i}\" for i in range(X_train_vectors.shape[1])]\n",
    "    all_feature_names = tfidf_features + count_features + embedding_features + feature_names\n",
    "    feature_importance = rf_model.feature_importances_\n",
    "    top_k = min(20, feature_importance.shape[0])\n",
    "    top_indices = np.argsort(feature_importance)[-top_k:]\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    plt.barh(range(top_k), feature_importance[top_indices])\n",
    "    plt.yticks(range(top_k), [all_feature_names[i] for i in top_indices])\n",
    "    plt.xlabel('‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç‡∏Ç‡∏≠‡∏á Feature'); plt.title(f'{top_k} Features ‡∏ó‡∏µ‡πà‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç‡∏ó‡∏µ‡πà‡∏™‡∏∏‡∏î (Random Forest)')\n",
    "    plt.tight_layout(); plt.savefig(\"feature_importance.png\", dpi=300, bbox_inches='tight'); plt.show()\n",
    "\n",
    "### **19. ‡∏™‡∏£‡∏∏‡∏õ‡∏õ‡∏£‡∏∞‡∏™‡∏¥‡∏ó‡∏ò‡∏¥‡∏†‡∏≤‡∏û**\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"‡∏™‡∏£‡∏∏‡∏õ‡∏õ‡∏£‡∏∞‡∏™‡∏¥‡∏ó‡∏ò‡∏¥‡∏†‡∏≤‡∏û‡∏™‡∏∏‡∏î‡∏ó‡πâ‡∏≤‡∏¢\")\n",
    "print(\"=\"*60)\n",
    "summary_df = pd.DataFrame({\n",
    "    'Model': results_df['Model'],\n",
    "    'Test Accuracy': results_df['Test Accuracy'],\n",
    "    'CV Accuracy': results_df['CV Mean'],\n",
    "    'Std Dev': results_df['CV Std'],\n",
    "    'Training Time': results_df['Training Time (s)'],\n",
    "    'Rank': results_df['Test Accuracy'].rank(ascending=False).astype(int)\n",
    "}).sort_values('Test Accuracy', ascending=False)\n",
    "print(summary_df.round(4))\n",
    "\n",
    "print(f\"\\nüéØ ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÄ‡∏ä‡∏¥‡∏á‡∏•‡∏∂‡∏Å‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç:\")\n",
    "print(f\"‚Ä¢ ‡πÇ‡∏°‡πÄ‡∏î‡∏•‡∏ó‡∏µ‡πà‡∏î‡∏µ‡∏ó‡∏µ‡πà‡∏™‡∏∏‡∏î: {best_model_name} (‡∏Ñ‡∏ß‡∏≤‡∏°‡πÅ‡∏°‡πà‡∏ô‡∏¢‡∏≥: {results[best_model_name]['accuracy']:.4f})\")\n",
    "print(f\"‚Ä¢ ‡∏Å‡∏≤‡∏£‡πÉ‡∏ä‡πâ FastText + Thai2Vec embeddings ‡∏ä‡πà‡∏ß‡∏¢‡∏à‡∏±‡∏î‡∏Å‡∏≤‡∏£‡∏†‡∏≤‡∏©‡∏≤‡∏ú‡∏™‡∏°‡πÑ‡∏î‡πâ‡∏î‡∏µ‡∏Ç‡∏∂‡πâ‡∏ô\")\n",
    "print(f\"‚Ä¢ ‡πÇ‡∏°‡πÄ‡∏î‡∏•‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡∏à‡∏±‡∏î‡∏Å‡∏≤‡∏£‡∏Ñ‡∏≥‡∏†‡∏≤‡∏©‡∏≤‡∏≠‡∏±‡∏á‡∏Å‡∏§‡∏©‡πÉ‡∏ô‡∏£‡∏µ‡∏ß‡∏¥‡∏ß‡πÑ‡∏ó‡∏¢‡πÑ‡∏î‡πâ‡πÅ‡∏•‡πâ‡∏ß\")\n",
    "print(f\"‚Ä¢ Features ‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î: {X_train_combined.shape[1]:,}\")\n",
    "print(f\"‚Ä¢ ‡∏£‡∏≠‡∏á‡∏£‡∏±‡∏ö‡∏ó‡∏±‡πâ‡∏á‡∏£‡∏µ‡∏ß‡∏¥‡∏ß‡∏†‡∏≤‡∏©‡∏≤‡πÑ‡∏ó‡∏¢‡πÅ‡∏•‡∏∞‡∏≠‡∏±‡∏á‡∏Å‡∏§‡∏©\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
